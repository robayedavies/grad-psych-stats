<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Psychological data analysis for graduate students - 6&nbsp; Introduction to Generalized Linear Mixed-effects Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./intro.html" rel="next">
<link href="./03-mixed.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction to Generalized Linear Mixed-effects Models</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Psychological data analysis for graduate students</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">MAKING THE MOST OF R</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./knowledge-ecosystem.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">R knowledge</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./visualization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data visualization</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">MODELS</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-multilevel.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to multilevel data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-mixed.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introduction to linear mixed-effects models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-mixed.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Developing linear mixed-effects models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-glmm.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction to Generalized Linear Mixed-effects Models</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">WRITING ABOUT RESEARCH</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction: the why</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./what.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">What</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./how.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">How</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">END</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-glmm-ideas" id="toc-sec-glmm-ideas" class="nav-link active" data-scroll-target="#sec-glmm-ideas"><span class="toc-section-number">6.1</span>  The key idea to get us started</a></li>
  <li><a href="#sec-glmm-targets" id="toc-sec-glmm-targets" class="nav-link" data-scroll-target="#sec-glmm-targets"><span class="toc-section-number">6.2</span>  Targets</a></li>
  <li><a href="#sec-glmm-guide" id="toc-sec-glmm-guide" class="nav-link" data-scroll-target="#sec-glmm-guide"><span class="toc-section-number">6.3</span>  Study guide</a></li>
  <li><a href="#sec-glmm-motivations" id="toc-sec-glmm-motivations" class="nav-link" data-scroll-target="#sec-glmm-motivations"><span class="toc-section-number">6.4</span>  Motivations</a>
  <ul class="collapse">
  <li><a href="#our-focus-is-on-the-analysis-of-catgorical-outcome-variables" id="toc-our-focus-is-on-the-analysis-of-catgorical-outcome-variables" class="nav-link" data-scroll-target="#our-focus-is-on-the-analysis-of-catgorical-outcome-variables"><span class="toc-section-number">6.4.1</span>  Our focus is on the analysis of catgorical outcome variables</a></li>
  <li><a href="#recognize-the-limitations-of-alternative-methods-for-analyzing-response-accuracy" id="toc-recognize-the-limitations-of-alternative-methods-for-analyzing-response-accuracy" class="nav-link" data-scroll-target="#recognize-the-limitations-of-alternative-methods-for-analyzing-response-accuracy"><span class="toc-section-number">6.4.2</span>  Recognize the limitations of alternative methods for analyzing response accuracy</a></li>
  </ul></li>
  <li><a href="#sec-glmm-practical-understanding" id="toc-sec-glmm-practical-understanding" class="nav-link" data-scroll-target="#sec-glmm-practical-understanding"><span class="toc-section-number">6.5</span>  Understanding the Generalized part of the Generalized Linear Mixed-effects Models in practical terms</a></li>
  <li><a href="#sec-glmm-word-learning-data" id="toc-sec-glmm-word-learning-data" class="nav-link" data-scroll-target="#sec-glmm-word-learning-data"><span class="toc-section-number">6.6</span>  The data we will work with: the Ricketts word learning study</a>
  <ul class="collapse">
  <li><a href="#sec-glmm-data-study" id="toc-sec-glmm-data-study" class="nav-link" data-scroll-target="#sec-glmm-data-study"><span class="toc-section-number">6.6.1</span>  Study information</a></li>
  <li><a href="#sec-glmm-data-download" id="toc-sec-glmm-data-download" class="nav-link" data-scroll-target="#sec-glmm-data-download"><span class="toc-section-number">6.6.2</span>  Locate and download the data file</a></li>
  </ul></li>
  <li><a href="#sec-glmm-data-tidy" id="toc-sec-glmm-data-tidy" class="nav-link" data-scroll-target="#sec-glmm-data-tidy"><span class="toc-section-number">6.7</span>  Tidy the data</a>
  <ul class="collapse">
  <li><a href="#sec-glmm-data-import" id="toc-sec-glmm-data-import" class="nav-link" data-scroll-target="#sec-glmm-data-import"><span class="toc-section-number">6.7.1</span>  Read-in the data file using read_csv</a></li>
  <li><a href="#sec-glmm-data-recode" id="toc-sec-glmm-data-recode" class="nav-link" data-scroll-target="#sec-glmm-data-recode"><span class="toc-section-number">6.7.2</span>  Code categorical factors</a></li>
  </ul></li>
  <li><a href="#sec-glmm-working" id="toc-sec-glmm-working" class="nav-link" data-scroll-target="#sec-glmm-working"><span class="toc-section-number">6.8</span>  Working with GLMMs in R</a>
  <ul class="collapse">
  <li><a href="#sec-glmm-random-intercepts" id="toc-sec-glmm-random-intercepts" class="nav-link" data-scroll-target="#sec-glmm-random-intercepts"><span class="toc-section-number">6.8.1</span>  Specify a random intercepts model</a></li>
  <li><a href="#sec-glmm-read-results" id="toc-sec-glmm-read-results" class="nav-link" data-scroll-target="#sec-glmm-read-results"><span class="toc-section-number">6.8.2</span>  Read the results</a></li>
  <li><a href="#sec-glmm-hypothesis" id="toc-sec-glmm-hypothesis" class="nav-link" data-scroll-target="#sec-glmm-hypothesis"><span class="toc-section-number">6.8.3</span>  GLMMs and hypothesis tests</a></li>
  <li><a href="#sec-glmm-visualizing" id="toc-sec-glmm-visualizing" class="nav-link" data-scroll-target="#sec-glmm-visualizing"><span class="toc-section-number">6.8.4</span>  Presenting and visualizing the effects</a></li>
  </ul></li>
  <li><a href="#sec-glmm-random-effects" id="toc-sec-glmm-random-effects" class="nav-link" data-scroll-target="#sec-glmm-random-effects"><span class="toc-section-number">6.9</span>  Examining if we should include random effects</a>
  <ul class="collapse">
  <li><a href="#sec-glmm-comparing-models" id="toc-sec-glmm-comparing-models" class="nav-link" data-scroll-target="#sec-glmm-comparing-models"><span class="toc-section-number">6.9.1</span>  Examine the utility of random effects by comparing models with the same fixed effects but varying random effects</a></li>
  <li><a href="#sec-glmm-bad-signs" id="toc-sec-glmm-bad-signs" class="nav-link" data-scroll-target="#sec-glmm-bad-signs"><span class="toc-section-number">6.9.2</span>  Bad signs</a></li>
  <li><a href="#sec-glmm-comparing0-models-2" id="toc-sec-glmm-comparing0-models-2" class="nav-link" data-scroll-target="#sec-glmm-comparing0-models-2"><span class="toc-section-number">6.9.3</span>  Comparison of models varying in random effects</a></li>
  <li><a href="#sec-glmm-addressing-convergence" id="toc-sec-glmm-addressing-convergence" class="nav-link" data-scroll-target="#sec-glmm-addressing-convergence"><span class="toc-section-number">6.9.4</span>  Addressing convergence problems</a></li>
  </ul></li>
  <li><a href="#sec-glmm-reporting-results" id="toc-sec-glmm-reporting-results" class="nav-link" data-scroll-target="#sec-glmm-reporting-results"><span class="toc-section-number">6.10</span>  Reporting model results</a></li>
  <li><a href="#sec-glmm-summary" id="toc-sec-glmm-summary" class="nav-link" data-scroll-target="#sec-glmm-summary"><span class="toc-section-number">6.11</span>  Summary</a>
  <ul class="collapse">
  <li><a href="#sec-glmm-glossary-useful-functions" id="toc-sec-glmm-glossary-useful-functions" class="nav-link" data-scroll-target="#sec-glmm-glossary-useful-functions"><span class="toc-section-number">6.11.1</span>  Glossary: useful functions</a></li>
  </ul></li>
  <li><a href="#r-code-and-data-file-access-for-the-class" id="toc-r-code-and-data-file-access-for-the-class" class="nav-link" data-scroll-target="#r-code-and-data-file-access-for-the-class"><span class="toc-section-number">6.12</span>  R code and data file access for the class</a></li>
  <li><a href="#sec-glmm-recommended-reading" id="toc-sec-glmm-recommended-reading" class="nav-link" data-scroll-target="#sec-glmm-recommended-reading"><span class="toc-section-number">6.13</span>  Recommended reading</a>
  <ul class="collapse">
  <li><a href="#a-very-useful-faq" id="toc-a-very-useful-faq" class="nav-link" data-scroll-target="#a-very-useful-faq"><span class="toc-section-number">6.13.1</span>  A <em>very</em> useful FAQ</a></li>
  <li><a href="#references-list" id="toc-references-list" class="nav-link" data-scroll-target="#references-list"><span class="toc-section-number">6.13.2</span>  References list</a></li>
  </ul></li>
  <li><a href="#appendix-example-dataset-variable-information" id="toc-appendix-example-dataset-variable-information" class="nav-link" data-scroll-target="#appendix-example-dataset-variable-information"><span class="toc-section-number">6.14</span>  Appendix: Example dataset variable information</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-glmm-intro" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction to Generalized Linear Mixed-effects Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell">

</div>
<div class="cell">

</div>
<p>We have been discussing how we can use Linear Mixed-effects models to analyze multilevel structured data, the kind of data that we commonly acquire in experimental psychological studies, for example, when our studies have repeated measures designs. The use of Linear Mixed-effects models is appropriate where the outcome variable is a continuous numeric variable like reaction time. In this chapter, we extend our understanding and skills by moving to examine data where the outcome variable is categorical: this is a context that requires the use of <strong>Generalized Linear Mixed-effects Models (GLMMs)</strong>.</p>
<p>We will begin by looking at the motivations for using GLMMs. We will then look at a practical example of a GLMM analysis, in an exploration in which we shall reveal some of the challenges that can arise in such work. The R code to do the modeling is very similar to the code we have used before. The way we can understand the models is also similar but <em>with one critical difference</em>. We start to understand that difference here.</p>
<section id="sec-glmm-ideas" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="sec-glmm-ideas"><span class="header-section-number">6.1</span> The key idea to get us started</h2>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Categorical outcomes cannot be analyzed using linear models, in whatever form, without having to make some important compromises.</p>
</div>
</div>
<p>You need to do something about the categorical nature of the outcome.</p>
</section>
<section id="sec-glmm-targets" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="sec-glmm-targets"><span class="header-section-number">6.2</span> Targets</h2>
<p>In this chapter, we look at Generalized Linear Mixed-effects Models (GLMMs): we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g.&nbsp;correct vs.&nbsp;incorrect). Our aims are to:</p>
<ol type="1">
<li>Understand the reasons for using GLMMs when we analyze discrete outcome variables.</li>
<li>Recognize the limitations of alternative methods for analyzing such outcomes.</li>
<li>Practice running GLMMs with varying random effects structures.</li>
<li>Practice reporting the results of GLMMs, including through the use of model plots.</li>
</ol>
</section>
<section id="sec-glmm-guide" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-glmm-guide"><span class="header-section-number">6.3</span> Study guide</h2>
<ol type="1">
<li>Play with the .R file used to create examples for the lecture.</li>
<li>Edit example code to create alternate visualizations of variable distributions and of the relationships between critical variables.</li>
<li>Run generalized linear mixed-effects models of demonstration data.</li>
<li>Run generalized linear mixed-effects models of alternate data sets.</li>
</ol>
<p>You will see that in the references list at the end, I have recommended some papers that I think provide particularly useful or readable introductions to GLMMs.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04-glmm_files/figure-html/word-learning-lm-per-subject-for-report-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Monaghan et al.&nbsp;(2015) artificial word learning study: plot showing the proportion of responses correct for each participant, in each of 12 blocks of 24 learning trials, in each learning condition; each grey line shows the linear model prediction of the proportion correct, for each person, by learning block, in each condition; black lines show the average prediction of the proportion correct, by learning block, in each condition. The position of points has been jittered.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-glmm-motivations" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec-glmm-motivations"><span class="header-section-number">6.4</span> Motivations</h2>
<section id="our-focus-is-on-the-analysis-of-catgorical-outcome-variables" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="our-focus-is-on-the-analysis-of-catgorical-outcome-variables"><span class="header-section-number">6.4.1</span> Our focus is on the analysis of catgorical outcome variables</h3>
<p>We often need to analyze outcome or dependent variables which comprise observations of responses that are discrete or categorical. We need to learn to recognize research contexts that require GLMMs. Categorical outcome variables can include any of the following:</p>
<ul>
<li><p>The accuracy of responses: is a response correct or incorrect?</p></li>
<li><p>The location of a recorded eye movement, e.g.&nbsp;a fixation to the left or to the right visual field.</p></li>
<li><p>The membership of one group out of two possible groups: e.g., is a participant impaired or unimpaired.</p></li>
<li><p>The membership of one group out of multiple possible groups: e.g., is a participant a member of one out of some number of groups, say, a member of a religious or ethnic group; e.g., is an incorrect response one out of some number of possible error types?</p></li>
<li><p>Responses that can be coded in terms of ordered categories: e.g., a response on a (Likert) ratings scale.</p></li>
<li><p>Outcomes like the frequency of occurrence of an event, e.g., how many arrests are made at a particular city location?</p></li>
</ul>
<p>In this chapter, we will focus on <strong>accuracy data</strong>: where the outcome variable consists of responses observed in a behavioural task, the accuracy of responses was recorded, and responses could either be correct or incorrect. The accuracy of a response is, here, coded under a <strong>binary</strong> or dichotomous classification though we can imagine situations when a response is coded in multiple different ways.</p>
<p>Those interested in analyzing outcome data from ratings scales, that is, ordered categorical outcome variables, often called ordinal data, may wish to read about ordinal regression analyses, which you can do in R using functions from the <code>{ordinal}</code> <a href="https://cran.r-project.org/web/packages/ordinal/vignettes/clmm2_tutorial.pdf">library</a>.</p>
<p>Those interested in analyzing outcome data composed of counts may wish to read about poisson regression analyses in Gelman and Hill (2007).</p>
<p>It will be apparent in our discussion that researchers have used, and will continue to use, a number of ‘traditional’ methods to analyze categorical outcome variables when really they should be using GLMMs. We will talk about these alternatives first, so that you recognize what is being done when you read research articles. Critically, we will discuss the limitations of such methods because these limitations explain why we bother to learn about GLMMs.</p>
</section>
<section id="recognize-the-limitations-of-alternative-methods-for-analyzing-response-accuracy" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="recognize-the-limitations-of-alternative-methods-for-analyzing-response-accuracy"><span class="header-section-number">6.4.2</span> Recognize the limitations of alternative methods for analyzing response accuracy</h3>
<p>If you want to analyze data from a study where responses can be either correct or incorrect but not both (and not anything else), then your outcome variable is categorical, and your analysis approach ought to respect that. However, if you read enough psychological research articles then you will see many reports of data analyses in which the researchers collected data on the accuracy of responses but then present the results of analyses that <em>ignored</em> the binary or dichotomous nature of accuracy. We often see response accuracy analyzed using an approach that looks something like the following:</p>
<ol type="1">
<li>The accuracy of responses (correct vs.&nbsp;incorrect) is counted, e.g., as the number of correct responses or the number of errors.</li>
<li>The percentage, or the proportion, of responses that are correct or incorrect is calculated, for each participant, for each level of each experimental condition or factor.</li>
<li>The percentage or proportion values are then entered as the outcome or dependent variable in ANOVA or t-test or linear model (multiple regression) analyses of response accuracy.</li>
</ol>
<p>You will see many reports of ANOVA or t-test or linear model analyses of accuracy.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why can’t we follow these examples, and save ourselves the effort of learning how to use GLMMs?</p>
<ul>
<li>The reason is that these analyses are, at best, approximations to more appropriate methods.</li>
<li>Their results can be expected to be questionable, or misleading, for reasons that we discuss next.</li>
</ul>
</div>
</div>
<section id="accuracy-is-bounded-between-1-and-0-linear-model-model-predictions-or-confidence-intervals-are-not" class="level4" data-number="6.4.2.1">
<h4 data-number="6.4.2.1" class="anchored" data-anchor-id="accuracy-is-bounded-between-1-and-0-linear-model-model-predictions-or-confidence-intervals-are-not"><span class="header-section-number">6.4.2.1</span> Accuracy is bounded between 1 and 0, linear model model predictions or confidence intervals are not</h4>
<p>To illustrate the problems associated with using traditional analysis methods (like ANOVA or multiple regression), when working with accuracy as an outcome, we start by looking at data from an artificial vocabulary learning study (reported by Monaghan, Mattock, Davies, &amp; Smith, 2015). Monaghan et al.&nbsp;(2015) recorded responses made by participants to stimuli in a test where the response was correct (coded 1) or incorrect (coded 0). In our study, we directly compared learning of noun-object pairings, verb-motion pairings, and learning of both noun and verb pairings simultaneously, using a cross-situational learning task. (Those interested in this dataset can read more about it at the online repository associated with this chapter.) The data will have a multilevel structure because you will have multiple responses recorded for each person, and for each stimulus. But what concerns us is that if you attempt to use a linear model to analyze the effects of the experimental variables then you will see some paradoxical results that are easily demonstrated.</p>
<p>Let’s imagine that we wish to estimate the effects of experimental variables like learning condition: learning trial block (1-12); or vocabulary condition (noun-only, noun-verb, verb-only). We can calculate the proportion of responses correct made by each person for each condition and learning trial block. We can then plot the regression best fit lines indicating how proportion of responses correct varies by person and condition. Figure @ref(fig:word-learning-lm-per-subject-for-report) shows the results. <strong>Look at where the best fit lines go.</strong></p>
<p>Figure @ref(fig:word-learning-lm-per-subject-for-report) shows how variation in the outcome, here, the proportion of responses that are correct, is bounded between the y-axis limits of 0 and 1 while the best fit lines exceed those limits. Clearly, if you consider the accuracy of a person’s responses in any set of trials, for any condition in an experiment, the proportion of responses that can be correct can vary only between 0 (no responses are correct) and 1 (all responses are correct). There is no inbuilt or intrinsic limits to the proportion of responses that a <em>linear model</em> can predict would be correct. According to linear model predictions, if you follow the best fit lines in Figure @ref(fig:word-learning-lm-per-subject-for-report), there are conditions, or there are participants, in which the proportion of a person’s responses that could be correct will be <em>greater than 1</em>. That is impossible.</p>
</section>
<section id="anova-or-regression-require-the-assumption-of-homogeneity-of-variance-but-for-binary-outcomes-like-accuracy-the-variance-is-proportional-to-the-mean" class="level4" data-number="6.4.2.2">
<h4 data-number="6.4.2.2" class="anchored" data-anchor-id="anova-or-regression-require-the-assumption-of-homogeneity-of-variance-but-for-binary-outcomes-like-accuracy-the-variance-is-proportional-to-the-mean"><span class="header-section-number">6.4.2.2</span> ANOVA or regression require the assumption of homogeneity of variance but for binary outcomes like accuracy the variance is proportional to the mean</h4>
<p>The other fundamental problem with using analysis approaches like ANOVA or regression to analyze categorical outcomes like accuracy is that we cannot assume that the variance in accuracy of responses will be homogenous across different experimental conditions.</p>
<!-- Consider Figure \@ref(fig:jaeger-2008-fig-1-variance-prop-mean), which I have taken from Jaeger (2008), showing the way in which the variance in response accuracy will tend to, itself, vary between conditions in proportion to the average level of response accuracy in different conditions. -->
<!-- ```{r jaeger-2008-fig-1-variance-prop-mean, echo=FALSE, fig.cap="Jaeger (2008) variance of sample proportion dependent on probability that a response is correct (or probability that is 1 of 0,1 outcomes)", out.width = '40%'} -->
<!-- knitr::include_graphics("jaeger-2008-fig-1-variance-prop-mean.pdf") -->
<!-- ``` -->
<p>The logic of the problem can be set out as follows:</p>
<ol type="1">
<li>Given a binary outcome, e.g., where the response is correct or incorrect.</li>
<li>For every trial, there is a probability <span class="math inline">\(p\)</span> that the response is correct.</li>
<li>The variance of the proportion of trials (per condition) with correct responses is dependent on <span class="math inline">\(p\)</span> and greater when <span class="math inline">\(p \sim .5\)</span>, the probability that a response will be correct.</li>
</ol>
<p>Jaeger (2008; p.&nbsp;3) then explains the problem like this. <!-- % The expected sample proportion $p$ over $n$ trials is given by dividing $\mu_X$ by the number of trials $n$, equal to $p$. --> <!-- % % (as $\mu_X$ is equal to $np$).  --> <!-- % The variance of the sample proportion is a function of p: $\sigma^2_p = \frac{p(1-p)}{n}$ --> If the probability of a binomially distributed outcome like response accuracy differs between two conditions (call them conditions 1 and 2), the variances will only be identical if <span class="math inline">\(p1\)</span> (the proportion of correct responses in condition 1) and p2 (the proportion of correct responses in condition 1) are equally distant from 0.5 (e.g.&nbsp;<span class="math inline">\(p1 = .4\)</span> and <span class="math inline">\(p2 = .6\)</span>). The bigger the difference in distance from 0.5, comparing the conditions, the less similar the variances will be. <!-- Also, as can be seen in Figure \@ref(fig:jaeger-2008-fig-1-variance-prop-mean), d --></p>
<p>Differences close to 0.5 will matter less than differences closer to 0 or 1. Even if p1 and p2 are unequally distant from 0.5, as long as they are close to 0.5, the variances of the sample proportions will be similar. Sample proportions between 0.3 and 0.7 are considered close enough to 0.5 to assume homogeneous variances (Agresti, 2002).</p>
<!-- %Within this interval, $p(1-p)$ ranges from 0.21 for $p=.3$ or .7 to .25 for p = .5.  -->
<p>Unfortunately, we usually cannot determine a priori the range of sample proportions in our experiment. In general, variances in two binomially distributed conditions will not be homogeneous but, as you will recall, in both ANOVA and regression analysis, we assume homogeneity of variance in the outcome variable when we compare the effect of differences (in the mean outcome) between the different levels of a factor. This means that if we design a study in which the outcome variable is the accuracy of responses in different experimental conditions and we plan to use ANOVA or regression to estimate the effect of variation in experimental conditions on response accuracy then unless we get lucky} our estimation of the experimental effect will take place under circumstances in which the application of the analysis method (ANOVA or regression) and thus the analysis results will be invalid.</p>
</section>
<section id="summary-recognize-the-limitations-of-traditional-methods-for-analyzing-response-accuracy" class="level4" data-number="6.4.2.3">
<h4 data-number="6.4.2.3" class="anchored" data-anchor-id="summary-recognize-the-limitations-of-traditional-methods-for-analyzing-response-accuracy"><span class="header-section-number">6.4.2.3</span> Summary: Recognize the limitations of traditional methods for analyzing response accuracy</h4>
<p>The application of traditional (parametric) analysis methods like ANOVA or regression to categorical outcome variables like accuracy is very common in the psychological literature. The problem is that these approaches can give us misleading results.</p>
<!--  -->
<!--   \item -->
<!--   Linear models assume outcomes are unbounded so allow predictions that are impossible when outcomes are, in fact, bounded as is the case for accuracy or other categorical variables -->
<!--   \item -->
<!--   Linear models assume homogeneity of variance but that is unlikely and anyway cannot be predicted in advance when outcomes are categorical variables -->
<!--   \item -->
<!--   If we are interested in the effect of an interaction between two effects, using ANOVA or linear models on accuracy (proportions of responses correct) can tell you, wrongly, that the interaction is significant -->
<!--        -->
<p>Traditionally, researchers have recognized the limitations attached to using methods like ANOVA or regression to analyze categorical outcomes like accuracy and have applied remedies, transforming the outcome variables, e.g.&nbsp;the arcsine root transformation, to render them ‘more normal’. However, as Jaeger (2008) demonstrates, the remedies like the arcsine transformation that have traditionally been applied are often not likely to succeed. <!-- Jaeger (2008) completed a comparison of the results of analyses of accuracy data, where outcomes are raw values for the proportions of correct responses, or arcsine transformed values for proportions correct. --> <!-- His comparison demonsteated that the traditional techniques will show either that effects are significant when they are not or that effects are not significant when they are. --></p>
<!-- You can think about the problem like this. -->
<!-- Using ANOVA or regression to analyze the variation in a categorical outcome measure like the accuracy of responses is an *approximation*. -->
<!-- If you are lucky, and the proportions of responses that are correct are on average $\sim .5$ or, in different conditions, similar distances from $p \sim .5$ then the approximation will give you a reasonable estimate of the effect of your experimental conditions on your outcome measures. -->

<!-- If you are not interested in the accuracy of prediction of outcomes at the extremes, and so do not mind if your ordinary least squares regression (your normal linear model) of outcome accuracy can give you impossible predicted values (predicted proportions of correct responses $< 0$ or $>1$) then, likewise, the approximation is acceptable. -->
<!-- However, tolerating the limitations in these traditional approaches does seem hard to justify. -->
</section>
</section>
</section>
<section id="sec-glmm-practical-understanding" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="sec-glmm-practical-understanding"><span class="header-section-number">6.5</span> Understanding the Generalized part of the Generalized Linear Mixed-effects Models in practical terms</h2>
<p>What we need, then, is a method that allows us to analyze categorical outcomes. We find the appropriate method in Generalized Linear Models, and in Generalized Linear Mixed-effects Models for repeated measures or multilevel structured data. We can understand these methods, as their name suggests, as <em>generalizations</em> of linear models or linear mixed-effects models, generalizations that allow for the categorical nature of some outcome data. You can understand how Generalized Linear Mixed-effects Models work by seeing them as analyses of categorical outcome data like accuracy where the outcome variable is transformed, as I explain next (see Baguley, 2012, for a nice clear explanation).</p>
<p>Our problems begin with the need to estimate effects on a bounded outcome like accuracy with a linear model which, as we have seen, will yield unbounded predictions.</p>
<p>The logistic transformation takes <span class="math inline">\(p\)</span> the probability of an event with two possible outcomes, and turns it into a <strong>logit</strong>: the natural logarithm of the odds of the event. The effect of this transformation is to turn a discrete binary bounded outcome into a continuous unbounded outcome.</p>
<ol type="1">
<li>Transforming a probability to odds <span class="math inline">\(o = \frac{p}{1-p}\)</span> is a partial solution.</li>
</ol>
<ul>
<li>Odds are, for example, the ratio of the probability of the occurrence of an event compared to the probability of the non-occurrence of an event, or, in terms of a response accuracy variable, the ratio of the probability of the response being correct compared to the probability of the response being incorrect.</li>
<li>And odds are continuous numeric quantities that are scaled from zero to infinity.</li>
<li>You can see how this works if you run the calculations using the equation <span class="math inline">\(o = \frac{p}{1-p}\)</span> in R as <code>odds &lt;- p/(1-p)</code>, replacing p with various numbers (e.g.&nbsp;<span class="math inline">\(p = 0.1, 0.01, 0.001\)</span>).</li>
</ul>
<ol start="2" type="1">
<li>We can then use the (natural) logarithm of the odds <span class="math inline">\(logit = ln\frac{p}{1-p}\)</span> because using the logarithm removes the boundary at zero because log odds ranges from negative to positive infinity.</li>
</ol>
<ul>
<li>You can see how this works if you run the calculations using the equation <span class="math inline">\(logit = ln\frac{p}{1-p}\)</span> in R as <code>logit &lt;- log(p/(1-p))</code>, replacing p with smaller and smaller numbers (e.g.&nbsp;<span class="math inline">\(p = 0.1, 0.01, 0.001\)</span>) gets you increasing negative log odds.</li>
</ul>
<p>When we model the log odds (logit) that a response will be correct, the model is called a logistic regression} or logistic model. We can think of logistic models as working like linear models with log-odds outcomes.</p>
<p><span class="math display">\[
ln\frac{p}{1-p} = logitp = \beta_0 + \beta_1X_1 \dots
\]</span></p>
<ul>
<li>We can describe the predicted log odds of a response of one type as the sum of the effects</li>
<li>log odds range from negative to positive infinity (logit of 0 corresponds to proportion of .5)</li>
</ul>
<p>Baguley (2012) notes that is it advantageous that odds and probabilities are both directly interpretable. We are used to seeing and thinking in everyday life about the chances that some event will occur.</p>
<!-- In a logistic regression, the predictors have an additive relationship with respect to the log odds outcome, just like in an ordinary linear model. -->
<!-- \begin{figure}[h!] -->
<!--      \centering -->
<!--       \includegraphics[scale=0.55]{jaeger-2008-fig-2-logit-probability-space} -->
<!--       \caption{Jaeger (2008) the effect of some predictor x on a categorical outcome y: on the left the effect in logit space; on the right the effect in probability space} -->
<!--       \label{jaeger-2008-fig-2-logit-probability-space} -->
<!-- \end{figure}      -->
<!-- It is also useful to consider (Baguley, 2012) that the logistic transformation maps differences in the predictors onto a nonlinear function with a particular form, the sigmoid curve, see Figure \ref{jaeger-2008-fig-2-logit-probability-space}. -->
<!-- % -- to see the relationship between probability and predictors, can plot the inverse of the logistic function -- ie its cumulative distribution function p = \frac{e^x1 + e^x} --  -->
<!-- The curve has an almost linear section in the middle where a normal linear model might provide a good fit but curves sharply at the extremes. -->
<!-- %, reflecting the required non-linearity of effects at the boundaries 0 (no responses correct) and 1 (100\% responses correct). -->
<!-- % -- the curve produced by the inverse of the logistic function -- the function itself can be shifted up or down the x axis by adding a constant to the equation ie to the x eg the intercept in the systematic component of the linear model -- the curve can become steeper or shallower by multiplying the log odds by a constant eg changing the slope in a regression model -- changing the sign of the slope will change the direction of the slope so that the sigmoidal curve slopes down as it travels from left to right (required if the probability of success decreases as X increases) -->
<!-- Think about the curve like this. -->
<!-- We are modelling the probability that something is going to happen, as the log odds that a response will be correct (rather than incorrect). -->
<!-- Our best fit line is an S-shaped (sigmoid) curve, not a straight line like in ordinary linear models. -->
<!-- % The curve could move from left to right along the x-axis according to how our intercept varies. -->
<!-- Critically, the curve can be more or less steep depending on the coefficient of the effect of the experimental variable. -->
<!-- This means that the model can predict the log odds that a response will be of one kind (e.g. a correct response) given the coefficient of the estimated effect and the value of the experimental variable. -->
<!-- In the example we will discuss following, in concrete terms, we see how the log odds that a response in a learning trial increases as the number of learning trials that participants experienced increased. -->
<!-- % -- not sure that understand the following correctly but try to work it out here: -->
<!-- % -- "Logit models capture the fact that differences in proba- bilities around p = .5 matter less than the same changes close to 0 or 1." -->
<!-- % -- see following for attempt to work it out:- -->
<!-- % -- jaeger 2008 p 3 -- Also, as can be see in Fig. 1, differences close to 0.5 will matter less than differences closer to 0 or 1. -- this is because variance is dependent on p the probability of an outcome of one type, and that variance will be at maximum at p = .5 -->
<!-- % -- jaeger 2008 p4 -- Logit models capture the fact that differences in proba- bilities around p = .5 matter less than the same changes close to 0 or 1. This is illustrated in Fig. 2, where the left panel shows a hypothetical linear effect of a predictor x in logit space (y = 􏰆3 + 0.2x), and the right panel shows the same effect in probability space. As can be seen in the right panel, small changes on the x-axis around p = .5 (i.e. x=15 since 0=􏰆3+0.2*15=logit(0.5)) lead to large decreases or increases in probabilities compared to the same change on the x-axis closer to 0 or 1. -->
<!-- % -- what we can understand, then, is that: -->
<!-- % 1. small changes in x where p = .5 lead to large changes in probability -- the slope is steepest (figure 2, right) there -->
<!-- % 2. these changes will be less important -- the variance is greatest (figure 1) there -->
<!-- % -- 040316 -- maybe the point is that probability distance is stretched out in the middle but compact at the ends, like the distortion of space by -->
<!-- % gravity -->
<!-- What does it tell us that the predicted probability of a categorical outcome takes a sigmoid curve shape? -->
<!-- %{What does the curve tell us?} -->
<!--  -->
<!-- \item -->
<!-- Changes in predictors are associated with larger differences in probability when $p \sim .5$ where slope is steepest (Figure \ref{jaeger-2008-fig-2-logit-probability-space}) -->
<!-- \item -->
<!-- Variance in sample proportions are also greatest around p = .5 (Figure \ref{jaeger-2008-fig-1-variance-prop-mean}) so maybe we see bigger differences there but less importance? -->
<!--  -->
<!-- In my experience, what this amounts to is the fact that the accuracy of participants' responses is greatest when the average accuracy of responses in a conditions is about 50\%, when $p \sim .5$. -->
<!-- This greater variability allows room, as it were, for there to be effects of experimental conditions. -->
<!-- At the same time, it may mean that those effects are a bit harder to detect because we need to be locating differences between conditions against a greater background variability in responses. -->
<!-- If we are recording the accuracy of responses towards the extremes, so that performance is near ceiling $p \sim 1$ when participants are getting nearly all their responses correct or near floor $p \sim 0$ when participants are getting nearly all their responses wrong, the variability of the accuracy of responses is less. -->
<!-- You find that a task can be too easy, so every participant gets every response (or nearly every response) correct. -->
<!-- Or that a task can be too hard, so every participant gets every response (or nearly every response) wrong. -->
<!-- There may be then less variability in the accuracy of responses but you are also going to see that experimental variables, like differences between conditions, will have less of an effect. -->
<!-- You can think about the S-shape of the probability curve as reflecting the fact that an experimental variable will have more impact on the probability that a response will be of one kind (e.g. a correct response) for values of the variable when $p \sim .5$ and will have less and less impact on the probability of the response being correct for values of the experimental variable when $p \sim 0$ or $p \sim 1$. -->
<!-- In terms of experimental science, we often think or talk about performance in different conditions being at floor} or at ceiling}. -->
<!-- We generally prefer to examine the impacts of experimental variables well away from these extremes. -->
<!-- This is because the heterogeneity of variance in outcomes, discussed earlier, makes it difficult to come up with a valid interpretation of estimated effects when performance at one of the conditions is at floor or at ceiling. -->
</section>
<section id="sec-glmm-word-learning-data" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="sec-glmm-word-learning-data"><span class="header-section-number">6.6</span> The data we will work with: the Ricketts word learning study</h2>
<p>We will be working with data collected for a study investigating word learning in children, reported by Ricketts, Dawson, and Davies (2021). You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.</p>
<p>We will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it the 2 x 2 factorial design beloved of psychologists. You will be able to generalize from our work this week to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.</p>
<p>However, our focus here is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.</p>
<section id="sec-glmm-data-study" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="sec-glmm-data-study"><span class="header-section-number">6.6.1</span> Study information</h3>
<p>I am going to present the study information in some detail, in part, to enable you to make sense of the analysis aims and results and, in part, so that we can simulate results reporting in a meaningful context.</p>
<section id="sec-glmm-data-background" class="level4" data-number="6.6.1.1">
<h4 data-number="6.6.1.1" class="anchored" data-anchor-id="sec-glmm-data-background"><span class="header-section-number">6.6.1.1</span> Introduction: the background for the study</h4>
<p>Vocabulary knowledge is essential for processing language in everyday life and it is vital that we know how to optimize vocabulary teaching. One strategy with growing empirical support is orthographic facilitation: children and adults are more likely to learn new spoken words that are taught with their orthography (visual word forms; for a systematic review, see Colenbrander, Miles &amp; Ricketts, 2019). Why might orthographic facilitation occur? Compared to spoken inputs, written inputs are less transient across time and less variable across contexts. In addition, orthography is more clearly marked (e.g., the ends of letters and words) than the continuous speech stream. Therefore, orthographic forms may be more readily learned than phonological forms, providing a more effective ‘anchoring device’ (Ehri, 2014; Krepel, de Bree, &amp; de Jong, 2020), or hook, on which to hang semantic information.</p>
<!-- In research, the presence of orthography has usually been incidental, with two exceptions (Chambré, Ehri, \& Ness, 2017; Mengoni, Nash, \& Hulme, 2013).  -->
<!-- In Mengoni et al.'s study, the presence of orthography was explicit, with all children alerted to the spelling patterns for all items.  -->
<!-- The recent study by Chambré et al. (2017) investigated orthographic facilitation in beginner readers. -->
<!-- For one group, attention was directed to print but for the other group it was not.  -->
<!-- Directing attention to print did not enhance the observed orthographic facilitation effect. -->
<p>Ricketts et al.&nbsp;(2021) investigated how school-aged children learn words. We conducted two studies in which children learned phonological forms and meanings of 16 polysyllabic words in the same experimental paradigm. To test whether orthographic facilitation would occur, half of the words were taught with access to the orthographic form (orthography present condition) and the other half were taught without orthographic forms (orthography absent condition). In addition, we manipulated the instructions that children received: approximately half of the children were told that some words would appear with their written form (explicit group); the remaining children did not receive these instructions (incidental group). Finally, we investigated the impact of spelling-sound consistency of word targets for learning, by including words that varied continuously on a measure of pronunciation consistency (after Mousikou, Sadat, Lucas, &amp; Rastle, 2017).</p>
<p>The quality of lexical representations was measured in two ways. A cuing hierarchical response task (definition, cued definition, recognition) was used to elicit semantic knowledge from the phonological forms, providing a fine-grained measure of semantic learning. A spelling task indexed the extent of orthographic learning for each word. We focus on the analysis of the spelling task responses.</p>
<p>Ricketts et al.&nbsp;(2021) reported two studies. We focus on Study 1, in which Ricketts et al.&nbsp;measured knowledge of newly learned words at two intervals: first one week and then, again, eight months after training. Longitudinal studies of word learning are rare and this is the first longitudinal investigation of orthographic facilitation.</p>
<p>We addressed three research questions.</p>
</section>
<section id="sec-glmm-data-research-questions" class="level4" data-number="6.6.1.2">
<h4 data-number="6.6.1.2" class="anchored" data-anchor-id="sec-glmm-data-research-questions"><span class="header-section-number">6.6.1.2</span> Research questions</h4>
<ol type="1">
<li>Does the presence of orthography promote greater word learning? We predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).</li>
<li>Will orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching? We expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.</li>
<li>Does word consistency moderate the orthographic facilitation effect? For orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.</li>
</ol>
</section>
<section id="sec-glmm-data-study-design" class="level4" data-number="6.6.1.3">
<h4 data-number="6.6.1.3" class="anchored" data-anchor-id="sec-glmm-data-study-design"><span class="header-section-number">6.6.1.3</span> Design</h4>
<p>Children were taught 16 novel words in a <span class="math inline">\(2 \times 2\)</span> factorial design. The presence of orthography (orthography absent vs.&nbsp;orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs.&nbsp;explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.</p>
</section>
<section id="sec-glmm-data-participants" class="level4" data-number="6.6.1.4">
<h4 data-number="6.6.1.4" class="anchored" data-anchor-id="sec-glmm-data-participants"><span class="header-section-number">6.6.1.4</span> Participants</h4>
<p>In Study 1, 41 children aged 9-10 years completed the word learning task and completed semantic and orthographic assessments one week after learning (Time 1), and eight months later (Time 2). We tested children from one socially mixed school in the South-East of England (<span class="math inline">\(M_{age} = 9.95, SD = .53\)</span>).</p>
</section>
<section id="sec-glmm-data-materials" class="level4" data-number="6.6.1.5">
<h4 data-number="6.6.1.5" class="anchored" data-anchor-id="sec-glmm-data-materials"><span class="header-section-number">6.6.1.5</span> Stimulus materials</h4>
<p>Stimuli comprised 16 polysyllabic words, all of which were nouns. We indexed consistency at the whole word level using the H uncertainty statistic (after Mousikou et al., 2017; Treiman, Mullennix, Bijeljac-Babic, &amp; Richmond-Welty, 1995). <!-- The stimuli were read aloud by 33 children (17 girls, $M_{age} = 13.81 years, SD = .28$) recruited from a single school in the South-East of England, none of whom participated in the experiment. --> <!-- The frequency of each alternative pronunciation was recorded, and consistency was then calculated using the formula $H = \sum-pi \times log2(pi)$, where $pi$ is the proportion of participants giving a certain pronunciation.  --> An H value of 0 would indicate a consistent item (all participants producing the same pronunciation), with values <span class="math inline">\(&gt;0\)</span> indicating greater inconsistency (pronunciation variability) with increasing magnitude.</p>
<!-- ### Study One Methods -- Procedure -->
</section>
<section id="sec-glmm-data-procedure" class="level4" data-number="6.6.1.6">
<h4 data-number="6.6.1.6" class="anchored" data-anchor-id="sec-glmm-data-procedure"><span class="header-section-number">6.6.1.6</span> Procedure</h4>
<p>A pre-test was conducted to establish participants’ knowledge of the stimulus words before i.e.&nbsp;pre-} training was administered. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3). <!-- All children, in both studies 1 and 2 completed the Session 3 post-tests. --> In Study 1, longitudinal post-test data were collected because children were post-tested at two time points. (Here, we refer to “post-tests” as the tests done to test learning, after i.e.&nbsp;<em>post</em> training.) Children were given post-tests in Session 3, as noted: this was Time 1. They were then given post-tests again, about eight months later at Time 2. <!-- In Study 2, the Study 1 sample was combined with an older sample of children. --> <!-- The additional Study 2 children were not tested at Time 2, and the analysis of Study 2 data did not incorporate test time as a factor. --></p>
</section>
<section id="sec-glmm-data-outcome" class="level4" data-number="6.6.1.7">
<h4 data-number="6.6.1.7" class="anchored" data-anchor-id="sec-glmm-data-outcome"><span class="header-section-number">6.6.1.7</span> Outcome (dependent) variables – Orthographic post-test</h4>
<p>This post-test was used to examine orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. For the purposes of our learning in Week 20, we focus on the accuracy of responses. Each response made by a child to a target word was coded as correct or incorrect.</p>
<p>Note that a more sensitive outcome measure of orthographic knowledge was also taken. Responses were also scored using a Levenshtein distance measure, using the `stringdist: library (van der Loo, 2019). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. <!-- The maximum score is 0, with higher scores indicating less accurate responses. For the interested reader, accuracy data are also available alongside Levenshtein distance scores in the shared data-sets. --> In the published report (Ricketts et al., 2021) we focus our analysis of the orthographic outcome on the Levenshtein distance measure of response spelling accuracy, and further details on the analysis approach (Poisson rather than Binomial Generalized Linear Mixed-effects Models) can be found in the paper.</p>
<!-- ### Background measures -->
<!-- Participants completed a set of background ability measures. -->
<!-- * **Nonverbal reasoning** was measured using the Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence – Second Edition (WASI-II; Wechsler, 2011), a pattern completion task.  -->
<!-- * **Word and nonword reading** were assessed using the Sight Word Efficiency (SWE) and Phonemic Decoding Efficiency (PDE) subtests of the Test of Word Reading Efficiency – Second Edition (TOWRE-2; Wagner, Torgesen, & Rashotte, 2011) and the Castles and Coltheart Test 2 (CC2; Castles et al., 2009).  -->
<!-- For the TOWRE-2 subtests, reading efficiency is indexed by the number of words (SWE) or nonwords (PDE) read correctly in 45 seconds.  -->
<!-- For the CC2, children were presented with a series of interleaved regular words, irregular words and nonwords (40 of each type) printed on individual cards, which they were asked to read aloud.  -->
<!-- * **Oral vocabulary knowledge** was indexed by the Vocabulary subtest of the WASI-II (Wechsler, 2011) and the British Picture Vocabulary Scale – Third Edition (BPVS-3; Dunn, Dunn, & NFER, 2009). -->
<!-- The WASI-II indexes expressive vocabulary by asking children to verbally define words.  -->
<!-- The BPVS-3 is a receptive vocabulary measure for which children are asked to indicate which of four pictures represents the meaning of each word. -->
</section>
</section>
<section id="sec-glmm-data-download" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="sec-glmm-data-download"><span class="header-section-number">6.6.2</span> Locate and download the data file</h3>
<p>Go to the 402 Moodle folder for Week 20, find and download the data file we need from the Week 20 resources folder:</p>
<p></p>
<p>Or download the resources folder, including the data file and associated scripts by clicking on the link:</p>
<p></p>
<p>We will be working with the data about the orthographic post-test outcome for the longitudinal study:</p>
<ul>
<li><code>long.orth_2020-08-11.csv</code></li>
</ul>
<p>Where <strong>long</strong> indicates the longitudinal nature of the data-set. The <code>.csv</code> file is a <em>comma separated values</em> file and can be opened in Excel.</p>
</section>
</section>
<section id="sec-glmm-data-tidy" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-glmm-data-tidy"><span class="header-section-number">6.7</span> Tidy the data</h2>
<p>The data are already <em>tidy</em>: each column in <code>long.orth_2020-08-11.csv</code> corresponds to a variable and each row corresponds to an observation. <!-- Because the design of the study involves the collection of repeated observations, the data can be understood to be in a *long* format. --> <!-- Each child was asked to respond to 16 words, and for each of the 16 words we collected post-test responses from multiple children. --> <!-- All words were presented to all children. --> <!-- Children in the longitudinal data set were given post-tests for each word twice, once at test time 1 and again at test time 2. --> <!-- This means that the data-set includes multiple rows of observations for each child: 32 rows for each child in the Study 1 longitudinal (test times 1 and 2) data. --> <!-- ; 16 rows for each child in the Study 2 (test time 1) data.  --></p>
<p>We will nevertheless be using <code>tidyverse</code> functions to prepare the data for analysis. We load the <code>tidyverse</code> and other helpful libraries here.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(here)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lme4)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MuMIn)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sjPlot)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="sec-glmm-data-import" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="sec-glmm-data-import"><span class="header-section-number">6.7.1</span> Read-in the data file using read_csv</h3>
<p>I am going to assume you have downloaded the data file, and that you know where it is. We use <code>read_csv</code> to read the data file into R.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>long.orth <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"long.orth_2020-08-11.csv"</span>, </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">col_types =</span> <span class="fu">cols</span>(</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Participant =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Time =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Study =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Instructions =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Version =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Word =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Orthography =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Measure =</span> <span class="fu">col_factor</span>(),</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Spelling.transcription =</span> <span class="fu">col_factor</span>()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>                      )</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>                    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can see, here, that within the <code>read_csv()</code> function call, I specify <code>col_types</code>, instructing R how to treat a number of different variables. You can read more about this <a href="https://readr.tidyverse.org/articles/readr.html">here</a>.</p>
<p>It is always a good to inspect what you have got when you read a data file in to R.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Some of the variables included in the <code>.csv</code> file are listed, following, with information about value coding or calculation.</p>
<ul>
<li><code>Participant</code> – Participant identity codes were used to anonymize participation.</li>
<li><code>Time</code> – Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.</li>
<li><code>Instructions</code> – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.</li>
<li><code>Word</code> – Letter string values showing the words presented as stimuli to the children.</li>
<li><code>Orthography</code> – Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.</li>
<li><code>Consistency-H</code> – Calculated orthography-to-phonology consistency value for each word. -<code>zConsistency-H</code> – Standardized Consistency H scores</li>
<li><code>Score</code> – Outcome variable – for the orthographic post-test, responses were scored as:</li>
</ul>
<ol type="1">
<li><code>1</code> – correct, if the target spelling was produced in full;</li>
<li><code>0</code> – incorrect, if the target spelling was not produced.</li>
</ol>
<p>The summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises. If you are interested in the dataset, you can find more information about the variables in the Appendix for this chapter and, of course, in Ricketts et al.&nbsp;(2021).</p>
</section>
<section id="sec-glmm-data-recode" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="sec-glmm-data-recode"><span class="header-section-number">6.7.2</span> Code categorical factors</h3>
<p>The data are tidy but we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables, the factors Orthography, Instructions, and Time. By default, R will <em>dummy code</em> observations at different levels of a factor. So, for a factor or a categorical variable like <code>Orthography</code> (present, absent), R will code one level name e.g.&nbsp;<code>absent</code> as <code>0</code> and the other e.g.&nbsp;<code>present</code> as <code>1</code>. The 0-coded level is termed the <em>reference level</em>, which you could call the baseline level, and by default R will code the level with the name appearing earlier in the alphabet as the reference level.</p>
<p>All this is usually not important. When you specify a model in R where you are asking to estimate the effect of a categorical variable like <code>Orthography</code> (present, absent) then, by default, what you will get is an estimate of the average difference in outcome, when all other factors are set to zero, estimated as the difference in outcomes comparing the reference level and the other level or levels of the factor. This will be presented, for example, like the output shown following, for a Generalized Linear Model (i.e., a logistic regression) analysis of the effect of Orthography condition, ignoring the random effects:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">glm</span>(Score <span class="sc">~</span> Orthography, <span class="at">family =</span> <span class="st">"binomial"</span>, <span class="at">data =</span> long.orth))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = Score ~ Orthography, family = "binomial", data = long.orth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8510  -0.8510  -0.6763   1.5436   1.7818  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -1.35879    0.09871 -13.765  &lt; 2e-16 ***
Orthographypresent  0.52951    0.13124   4.035 5.47e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1431.9  on 1262  degrees of freedom
Residual deviance: 1415.4  on 1261  degrees of freedom
AIC: 1419.4

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>You can see that you have an estimate, in the summary, of the effect of orthographic condition shown as:</p>
<p><code>Orthographypresent  0.52951</code></p>
<p>This model (and default coding) gives us an estimate of how the log odds of a child getting a response correct changes if we compare the responses in the <code>absent</code> condition (here, treated as the baseline or reference level) with responses in the <code>present</code> condition. (Notice that R tells us about the estimate by adding the name of the factor level that is <em>not</em> the reference level, here, <code>present</code> to the name of the variable <code>Orthography</code> whose effect is being estimated.) We can see that the log odds of a correct response increase by <span class="math inline">\(0.52951\)</span> when the orthography (visual word form or spelling) of a word is present during learning trials.</p>
<p>However, as Dale Barr explains <a href="http://talklab.psy.gla.ac.uk/tvw/catpred/">here</a> it is better not to use R’s default dummy coding scheme if we are analyzing data where the data come from a study involving two or more factors, and we want to estimate not just the main effects of the factors but also the effect of the interaction between the factors.</p>
<p>In our analyses, we want the coding that allows us to get estimates of the main effects of factors, and of the interaction effects, somewhat like what we would get from an ANOVA. This requires us to use effect coding.</p>
<p>We can code whether a response was recorded in the absent or present condition using numbers. In dummy coding, for any observation, we would use a column of zeroes or ones to code condition: i.e., absent (0) or present (1). In effect coding, for any observation, we would use a column of ones or minus ones to code condition: i.e., absent (-1) or present (1). (With a factor with more than two levels, we would use more than one column to do the coding: the number of columns we would use would equal the number of factor condition levels minus one.) In effect coding, observations coded -1 are in the reference level.</p>
<p>With effect coding, the constant (i.e., the intercept for our model) is equal to the grand mean of all the observed responses. And the coefficient of each of the effect variables is equal to the difference between the mean of the group coded 1 and the grand mean.</p>
<p>You can read more about effect coding <a href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-effect-coding/">here</a>.</p>
<!-- #### Why use effect coding? -->
<!-- I quote:  -->
<!-- \begin{quotation} -->
<!-- The primary benefit is that you get reasonable estimates of both the main effects and interaction using effect coding. With dummy coding the estimate of the interaction is fine but main effects are not "true" main effects but rather what are called simple effects, i.e., the effect of one variable at one level of the other variable. This is why most analysis of variance programs use some type of effect coding when estimating the various effects in an ANOVA model. -->
<!-- \end{quotation} -->
<!-- See for more information: -->
<!-- \url{https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-effect-coding/} -->
<!-- And see for further information: -->
<!-- \url{https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/} -->
<section id="sec-glmm-data-category-coding" class="level4" data-number="6.7.2.1">
<h4 data-number="6.7.2.1" class="anchored" data-anchor-id="sec-glmm-data-category-coding"><span class="header-section-number">6.7.2.1</span> Category coding practicalities</h4>
<p>We follow recommendations to use sum contrast coding for the experimental factors. Further, to make interpretation easier, we want the coding to work so that for both orthography and presentation conditions, doing something is the “high” level in the factor – hence:</p>
<ul>
<li>Orthography, absent (-1) vs.&nbsp;present (+1)</li>
<li>Instructions, incidental (-1) vs.&nbsp;explicit (+1)</li>
<li>Time, test time 1 (-1) vs.&nbsp;time 2 (+1)</li>
</ul>
<p>We use a modified version of the <code>contr.sum()</code> function (provided in the <code>{memisc}</code> library) that allows us to define the base or reference level for the factor manually (see <a href="https://www.rdocumentation.org/packages/memisc/versions/0.99.17.2/topics/contr">documentation</a>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(memisc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note: we have seen in some classes that we cannot appear to load <code>library(memisc)</code> and <code>library(tidyverse)</code> at the same time without getting weird warnings. So I would load <code>library(memisc)</code> after I have loaded <code>library(tidyverse)</code> and maybe unload it afterwards: just click on the button next to the package or library name in R-Studio to detach the library (i.e., stop it from being available in the R session).</p>
<p>In the following sequence, I first check how R codes the levels of each factor by default, I then change the coding, and check that the change gets me what I want.</p>
<p>We want effects coding for the orthography condition factor, with orthography condition coded as -1, +1. Check the coding.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Orthography)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        present
absent        0
present       1</code></pre>
</div>
</div>
<p>You can see that Orthography condition is initially coded, by default, using dummy coding: absent (0); present (1). We want to change the coding, then check that we have got what we want.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Orthography) <span class="ot">&lt;-</span> <span class="fu">contr.sum</span>(<span class="dv">2</span>, <span class="at">base =</span> <span class="dv">1</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Orthography)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         2
absent  -1
present  1</code></pre>
</div>
</div>
<p>We want effects coding for the presentation condition factor, with presentation condition coded as -1, +1. Check the coding.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Instructions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           incidental
explicit            0
incidental          1</code></pre>
</div>
</div>
<p>Change it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Instructions) <span class="ot">&lt;-</span> <span class="fu">contr.sum</span>(<span class="dv">2</span>, <span class="at">base =</span> <span class="dv">2</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Instructions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            1
explicit    1
incidental -1</code></pre>
</div>
</div>
<p>We want effects coding for the Time factor, with Time coded as -1, +1 Check the coding.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Time)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  2
1 0
2 1</code></pre>
</div>
</div>
<p>Change it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Time) <span class="ot">&lt;-</span> <span class="fu">contr.sum</span>(<span class="dv">2</span>, <span class="at">base =</span> <span class="dv">1</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(long.orth<span class="sc">$</span>Time)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   2
1 -1
2  1</code></pre>
</div>
</div>
</section>
<section id="code-tip" class="level4" data-number="6.7.2.2">
<h4 data-number="6.7.2.2" class="anchored" data-anchor-id="code-tip"><span class="header-section-number">6.7.2.2</span> Code tip</h4>
<p>I use <code>contr.sum(a, base = b)</code> to do the coding, where a is the number of levels in a factor, and b tells R which level to use as the baseline or reference level. I usually need to check the coding before and after I specify it.</p>
</section>
</section>
</section>
<section id="sec-glmm-working" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="sec-glmm-working"><span class="header-section-number">6.8</span> Working with GLMMs in R</h2>
<p>A small change in R <code>lmer</code> code allows us to extend what we know about linear mixed-effects models to conduct <em>Generalized Linear Mixed-effects Models</em>. We change the function call from <code>lmer()</code> to <code>glmer()</code>. However, we have to make some other changes, as we detail in the following sections.</p>
<p>We will be examining the impact of the experimental effects, that is, the fixed effects} associated with the impacts on the outcome <code>Score</code> (accuracy of response in the word spelling test) associated with the following comparisons:</p>
<ul>
<li>Time: time 1 versus time 2</li>
<li>Orthography: present versus absent conditions</li>
<li>Instructions: explicit versus incidental conditions</li>
<li>Standardized spelling-sound consistency</li>
<li>Interaction between the effects of Orthography and Instructions</li>
<li>Interaction between the effects of Orthography and consistency</li>
</ul>
<p>We will begin by keeping the random effects structure simple.</p>
<section id="sec-glmm-random-intercepts" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="sec-glmm-random-intercepts"><span class="header-section-number">6.8.1</span> Specify a random intercepts model</h3>
<p>In our first model, we will specify just random effects of participants and items on intercepts.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>long.orth.min.glmer <span class="ot">&lt;-</span> <span class="fu">glmer</span>(Score <span class="sc">~</span> </span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>                               Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span> </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                               Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>                               Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>                               (<span class="dv">1</span> <span class="sc">|</span> Participant) <span class="sc">+</span> </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>                               (<span class="dv">1</span> <span class="sc">|</span> Word),</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>                             <span class="at">family =</span> <span class="st">"binomial"</span>, </span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">glmerControl</span>(<span class="at">optimizer=</span><span class="st">"bobyqa"</span>, <span class="at">optCtrl=</span><span class="fu">list</span>(<span class="at">maxfun=</span><span class="fl">2e5</span>)),</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>                             <span class="at">data =</span> long.orth)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth.min.glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The code works as follows.</p>
<ul>
<li><code>glmer()</code> the function name changes because now we want a generalized} linear mixed-effects model of accuracy</li>
<li>With <code>(1 | Participant)</code> we include random effects of participants on on intercepts</li>
<li>With <code>(1 | Word)</code> we include random effects of stimulus on on intercepts</li>
<li><code>family = binomial</code> accuracy is a binary outcome variable (correct, incorrect) so we assume a binomial probability distribution</li>
<li><code>glmerControl(optimizer="bobyqa", ...)</code> we change the underlying mathematical engine (the optimizer) to cope with greater model complexity, and we allow the model fitting functions to take longer to find estimates with <code>optCtrl=list(maxfun=2e5)</code>.</li>
</ul>
<p>Notice how we specify the fixed effects. We want <code>glmer()</code> to estimate “main effects and interactions” that we hypothesized.</p>
<p>We specify the <em>main</em> effects with:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We specify the <em>interaction</em> effects with:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Where we ask for estimates of the fixed effects associated with:</p>
<ul>
<li><code>Orthography:Instructions</code> the interaction between the effects of Orthography and Instructions</li>
<li><code>Orthography:zConsistency-H</code> the interaction between the effects of Orthography and consistency</li>
</ul>
<section id="code-tip-1" class="level4" data-number="6.8.1.1">
<h4 data-number="6.8.1.1" class="anchored" data-anchor-id="code-tip-1"><span class="header-section-number">6.8.1.1</span> Code tip</h4>
<p>There are two forms of notation we can use to specify interactions in R. The simplest form is to use something like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>Orthography<span class="sc">*</span>Instructions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This will get you estimates of:</p>
<ul>
<li><code>Orthography</code> present versus absent conditions</li>
<li><code>Instructions</code> explicit versus incidental conditions</li>
<li><code>Orthography x Instructions</code> the interaction between the effects of Orthography and Instructions</li>
</ul>
<p>So, in general, if you want estimates of the effects of variables A, B and the interaction A x B, then you write <code>A*B</code>.</p>
<p>We can also use the colon symbol to specify only the interaction, i.e., ignoring main effects, so if you specify <code>A:B</code> then you will get an estimate of the interaction A x B but not the effects A, B. With the coding</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>Score <span class="sc">~</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> Orthography<span class="sc">:</span>Instructions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I would be making explicit that I want estimates for the effects of Orthography, Instruction and the interaction between the effects of Orthography and Instructions.</p>
</section>
</section>
<section id="sec-glmm-read-results" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="sec-glmm-read-results"><span class="header-section-number">6.8.2</span> Read the results</h3>
<p>If you run the model code, you will get the results shown in the output.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: Score ~ Time + Orthography + Instructions + zConsistency_H +  
    Orthography:Instructions + Orthography:zConsistency_H + (1 |  
    Participant) + (1 | Word)
   Data: long.orth
Control: glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e+05))

     AIC      BIC   logLik deviance df.resid 
  1040.4   1086.7   -511.2   1022.4     1254 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.0994 -0.4083 -0.2018  0.2019  7.4940 

Random effects:
 Groups      Name        Variance Std.Dev.
 Participant (Intercept) 1.840    1.357   
 Word        (Intercept) 2.224    1.491   
Number of obs: 1263, groups:  Participant, 41; Word, 16

Fixed effects:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -1.878462   0.443957  -4.231 2.32e-05 ***
Time2                        0.050136   0.083325   0.602    0.547    
Orthography2                 0.455009   0.086813   5.241 1.59e-07 ***
Instructions1                0.042289   0.230336   0.184    0.854    
zConsistency_H              -0.618093   0.384005  -1.610    0.107    
Orthography2:Instructions1   0.005786   0.083187   0.070    0.945    
Orthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1
Time2        0.008                                   
Orthogrphy2 -0.059  0.008                            
Instructns1  0.014  0.025  0.001                     
zCnsstncy_H  0.016 -0.002 -0.029 -0.001              
Orthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       
Orthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007</code></pre>
</div>
</div>
<p>In these results, we see:</p>
<ol type="1">
<li>First, information about the function used to fit the model, and the model object created by the <code>glmer()</code> function call</li>
</ol>
<ul>
<li>Then the model formula including main effects `Score ~ Time + Orthography + Instructions + zConsistencyH:</li>
<li>As well as interactions <code>Orthography:Instructions + Orthography:zConsistencyH</code></li>
<li>And the random effects <code>(1 | Participant) + (1 |Word)</code></li>
<li>Then we see <code>REML criterion at convergence</code> about the model fitting process, which we can usually ignore</li>
<li>Then we see information about the model algorithm</li>
<li>Then we see model fit statistics, including <code>AIC BIC logLik</code></li>
<li>Then we see information about the distribution of residuals</li>
</ul>
<ol start="2" type="1">
<li>Then the <code>Random Effects</code>. Notice that the statistics are <code>Variance Std.Dev.</code>, that is, the variance and the corresponding standard deviation associated with the random effects</li>
</ol>
<ul>
<li>The variance due to random differences between the average intercept (over all data) and the intercept for each participant</li>
<li>And the variance due to random differences between the average intercept (over all data) and the intercept for responses to each word stimulus</li>
</ul>
<ol start="3" type="1">
<li>Last, just as for linear models, we see estimates of the coefficients of the fixed effects, the intercept and the slopes of the experimental variables.</li>
</ol>
<p>Note that we get p-values for what are called Wald null hypothesis significance tests on the coefficients.</p>
<p>We can see that one effect is significant. The estimate for the effect of the presence compared to the absence of Orthography is <code>0.455009</code> The positive coefficient tells us that the log odds that a response will be correct is higher when Orthography is present compared to when it is absent.</p>
<p>We can also see an effect of consistency that can be considered to be marginal or near-significant by some standards. The estimate for the effect of <code>zConsistency_H</code> is <code>-0.618093</code> indicating that the log odds of a response being correct decrease for unit increase in the standardized H consistency measure.</p>
</section>
<section id="sec-glmm-hypothesis" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="sec-glmm-hypothesis"><span class="header-section-number">6.8.3</span> GLMMs and hypothesis tests</h3>
<p>As we discussed in the last chaptr, we can conduct null hypothesis significance tests by comparing models that differ in the presence or absence of a fixed effect or a random effect, using the Likelihood Ratio Test. In the results output for a GLMM by the `glmer(): function, you can see that alongside the estimates of the coefficients (and standard error) for the fixed effects we also have z and p-values. Wald z tests for GLMMs test the null hypothesis of no effect by comparing the effect estimate with their standard error, and comparing the resulting test statistic to zero (Bolker et al., 2009).</p>
</section>
<section id="sec-glmm-visualizing" class="level3" data-number="6.8.4">
<h3 data-number="6.8.4" class="anchored" data-anchor-id="sec-glmm-visualizing"><span class="header-section-number">6.8.4</span> Presenting and visualizing the effects</h3>
<p>We usually want to do more than just report whether experimental effects are or are not significant. It helps us to present and interpret the estimates from a model if we can visualize the model prediction. There are a variety of tools that help us to do this.</p>
<section id="sjplot-library" class="level4" data-number="6.8.4.1">
<h4 data-number="6.8.4.1" class="anchored" data-anchor-id="sjplot-library"><span class="header-section-number">6.8.4.1</span> sjPlot library</h4>
<p>We can use the <code>plot_model</code> function from the <code>{sjPlot}</code> library. The following sequence of code takes information from the model we have just run, then generates model predictions, of change in the probablity of a correct response (<code>Score</code>) for different levels of the Orthography factor and the consistency variable. I chose these variables because they are the significant or near-significant effects.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>porth <span class="ot">&lt;-</span> <span class="fu">plot_model</span>(long.orth.min.glmer,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">type=</span><span class="st">"pred"</span>,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">terms =</span> <span class="st">"Orthography"</span>) <span class="sc">+</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>         <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ggtitle</span>(<span class="st">"Predicted probability"</span>) <span class="sc">+</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>pzconsH <span class="ot">&lt;-</span> <span class="fu">plot_model</span>(long.orth.min.glmer,</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">type=</span><span class="st">"pred"</span>,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">terms =</span> <span class="st">"zConsistency_H"</span>) <span class="sc">+</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>         <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ggtitle</span>(<span class="st">"Predicted probability"</span>) <span class="sc">+</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(porth, pzconsH,</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>            <span class="at">ncol=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04-glmm_files/figure-html/sjplot-orthography-1.png" class="img-fluid figure-img" style="width:95.0%"></p>
<p></p><figcaption class="figure-caption">Effect of orthography condition (present versus absent) on probability of a response being correct</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The plots in Figure @ref(fig:sjplot-orthography) show clearly how the probability of a correct response is greater for the conditions where Orthography had been present (versus absent) during the word learning phase of the study. We can also see a trend such that the probability of a response being correct decreases as the (in-)consistency of a target word tends to increase.</p>
<section id="code-tip-2" class="level5" data-number="6.8.4.1.1">
<h5 data-number="6.8.4.1.1" class="anchored" data-anchor-id="code-tip-2"><span class="header-section-number">6.8.4.1.1</span> Code tip</h5>
<p>You will need to install the <code>{sjPlot}</code> library first, and then run the command <code>library(sjPlot)</code> before creating your plot.</p>
<p>Notice:</p>
<ul>
<li><code>plot_model()</code> produces the plot</li>
<li><code>plot_modellong.orth.min.glmer)</code> specifies that the plot should be produced given information about the previously fitted model <code>long.orth.min.glmer</code></li>
<li><code>type = "pred"</code> tells R that you want a plot showing the model predictions, of the effect of, e.g., `Orthography: condition</li>
</ul>
<p>The function outputs an object whose appearance can be edited as a <code>ggplot</code> object.</p>
<p>See the following set of blog posts for detailed advice and explanation:</p>
<p></p>
<p></p>
<p></p>
<p>And see the library manual for detailed technical <a href="https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf">information</a>.</p>
<!-- #### effects library -->
<!-- We can use the `effect: function from the `effects: library. -->
<!-- Again, we feed information about the model into a function that generates and plots model predictions, given variation in the named experimental factors. -->
<!-- ```{r effects-orthography, warning = FALSE, message = FALSE, fig.height=3.5, fig.width=6, out.width='95%', fig.cap="Effect of orthography condition (present versus absent) on probability of a response being correct"} -->
<!-- porth <- plot(effect("Orthography", mod = long.orth.min.glmer)) -->
<!-- pzconsH <- plot(effect("zConsistency_H", mod = long.orth.min.glmer)) -->
<!-- grid.arrange(porth, pzconsH, -->
<!--             ncol=2) -->
<!-- ``` -->
<!-- ##### Code tip -->
<!-- You will need to install the `effects: library first, and then run the command `library(effects): before creating your plot. -->
<!-- Notice: -->
<!--  -->
<!-- \item -->
<!-- `effect(): creates the information for the plot, given information about the fitted model -->
<!-- \item -->
<!-- `plot(effect(): embedding `effect(): within the `plot(): function call requires R to make a plot out of the information constructed from the model using the `effect(): function -->
<!-- \item -->
<!-- `effect("Orthography" ...): tells R what effect we want to plot -->
<!-- \item -->
<!-- `mod = long.orth.min.glmer: specifies that the model that is the basis for plot production -->
<!--  -->
<!-- This plotting function is based on what is called the lattice} plotting engine. -->
<!-- The lattice} plotting engine is a bit more flexible than the base R} engine. -->
<!-- \item -->
<!-- `rug = FALSE: I switch off the option to show the occurrence of observations at different values of the x-axis variable (here, block) though, in other contexts, this option is useful -->
<!-- \item -->
<!-- `ylab = "Probability (correct response)": I change the axis labelling to make it easier to understand -->
<!-- \item -->
<!-- `par.settings = list(fontsize = list(text = 15, points = 10))": I change the font size settings to make the labels a bit clearer -->
<!-- \item -->
<!-- I enclose the plotting code within the lines -->
<!--  -->
<!-- \item -->
<!-- `pdf("noun-verb-learning-glmm3-partial-effects-effects-plot.pdf", w=5, h=5): to create a .pdf file, named in `"":, of dimensions 6 inches width x 5 inches height -->
<!-- \item -->
<!-- `dev.off(): to close the file -->
<!--  -->
<!-- You can check the `effects: library manual for details. -->
<!-- \url{https://cran.r-project.org/web/packages/effects/effects.pdf} -->
<!-- See also: -->
<!-- \url{https://www.jstatsoft.org/article/view/v008i15} -->
<!-- You will need to install the `effects: library first, and then run the command `library(effects):. -->
<!-- \subsection{Exercises} -->
<!-- The plotting code outlined in the previous sections are relatively simple to use. -->
<!-- You can get good results, for your own analyses, by simply swapping the names of the example model and predictor variables for your own model name and your own analysis predictor variables. -->
<!-- However, you will find it helpful if you: -->
<!--  -->
<!-- \item -->
<!-- Take a look at the library information at the websites I have listed, and at other websites -->
<!-- \item -->
<!-- Examine what happens if you make different choices than I make, for example, by flipping `TRUE: to `FALSE: or `FALSE: to `TRUE: in the code -->
<!-- \item -->
<!-- Go deeper and vary plot settings relating to the composition and the appearance of the plot: labelling, label placement, label text size, theme, colour, line type etc. -->
<!--  -->
</section>
</section>
</section>
</section>
<section id="sec-glmm-random-effects" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="sec-glmm-random-effects"><span class="header-section-number">6.9</span> Examining if we should include random effects</h2>
<p>So far, we have been considering the results of a random intercepts} model in which we take into account the random effects of participants and stimulus word differences on intercepts. We have ignored the possibility that the slopes of the experimental variables might vary between participants or between words. We now need to examine the question: What random effects should we specify?</p>
<p>I must warn you that the question and the answer are complex but the coding is quite simple, and the approaches you can take to address the question are, now, quite well recognized by the psychological community. In other words, the community recognizes the nature of the problem, and recognizes the methods you can potentially follow to solve the problem.</p>
<p>The complexity, for us, lies not in the mathematics: the coding is simple and the `glmer(): function does the work. I think the complexity lies, firstly, in how we have to think about the study design, what gets manipulated or allowed to vary. I find it very helpful to sketch out, by hand, what the study design means in relation to who does what in an experiment. And when I work with collaborators, I typically ask them to explain the design verbally while I try to write in pencil what the dataset looks like (for a small number of participants or stimuli). The complexity lies, secondly, and in how we have to translate our understanding of the study design to a specification of random effects. We can master that} aspect of the challenge through practice.</p>
<section id="getting-started" class="level4" data-number="6.9.0.1">
<h4 data-number="6.9.0.1" class="anchored" data-anchor-id="getting-started"><span class="header-section-number">6.9.0.1</span> Getting started</h4>
<p>So, what random effects should we include? If you go back to the description of the study design, then you will be able to see that a number of possibilities follow, theoretically, from the design.</p>
<p>A currently influential set of recommendations (Barr et al., 2013; but see discussions in Bates et al., 2015; Matuschek et al., 2017; Meteyard &amp; Davies, 2020) has been labeled Keep it maximal}:</p>
<ul>
<li>If you are testing effects manipulated according to a pre-specified design then you should:</li>
<li>Test random intercepts – due to random differences between subjects or between items (or other sample grouping variables)</li>
<li>Test random slopes for all within-subjects or within-items (or other) fixed effects</li>
</ul>
<p>This means that specification of the random effects structure requires two sets of information:</p>
<ul>
<li>What are the fixed effects?</li>
<li>What are the grouping variables: did you test multiple participants using multiple stimuli (e.g., words …) or did you test participants under multiple different conditions (e.g., levels of experimental condition factors)?</li>
</ul>
<p>Our answers to these questions then dictate potentially how we specify the random effects structure for our model.</p>
<ul>
<li>We can consider that we should certainly include a random effect of each grouping variable (e.g., participants, stimulus word) on intercepts.</li>
<li>We can reflect that we should also include a random effect of a grouping variable e.g.&nbsp;<code>(|participant)</code> on the slope of each variable that was manipulated <em>within</em> the units of that variable.</li>
<li>When we specify models, we should remember that by default if you specify <code>(1 + something | participant)</code> then you are specifying that you want to take into account variance due to the random differences between participants in intercepts <code>(1 ...| participant)</code>, plus variance due to the random differences between participants in slopes <code>(... something | participant)</code>, plus the covariance (or correlation) between the random intercepts and the random slopes.</li>
</ul>
<!-- If a fixed effect is manipulated within-units then random effects of those units on intercepts and on the slopes of the within-units fixed effect should be examined but if it is manipulated between units then only random effects of those units on intercepts may be required. -->
<!-- So if a fixed effect (the effect of an experimental variable) is manipulated within-subjects, then it is often advisable to examine if there is a random effect of subjects on the slope of that fixed effect. -->
<!-- Likewise, if a fixed effect is manipulated within-items then it is advisable to examine if there is a random effect of items on the slope of that fixed effect. -->
<p>You will have become familiar with the practice of referring to effects as within-subjects} or between-subjects} previously, in working with ANOVA. Here, whether an effect is <em>within-subjects</em> or <em>within-items</em> or not has relevance to whether we can or should specify a random effect of subjects or of items on the slope of a fixed effect.</p>
<!--  -->
<!--    \item -->
<!--    It makes sense to allow for variation in the effects of variables that are manipulated within-subjects} -->
<!--     -->
<!--    \item -->
<!--    For example, if everyone sees the same words and you are testing the effect of word attributes then the effects of those word attribute variables are within-subjects} effects -->
<!--     -->
<!--   \item -->
<!--   Consider: you have 104 observations for a person, within} the variation in RT for that person the relative frequency of a word may or may not be associated with change in the RTs of their responses -->
<!--     -->
<!--     -->
<!--   \item -->
<!--   For the example in this chapter, every participant had to complete a series of learning trials and the trials were organised into blocks -->
<!--    -->
<!--   \item -->
<!--   We can say that the effect of block is a within-subjects} effect in the word learning study: we can compare the impact on accuracy of whether a person's response was recorded after they had experienced more or less blocks, for each person -->
<!--   \item -->
<!--   In comparison, if a varable is manipulated between different groups of participants it is a between-subjects} effect and we cannot see how the slope of the effect varied in different participants because no participant experienced all levels of the variable} -->
<!--    -->
<!--   \item -->
<!--    What about effects that may vary within-items}? -->
<!--     -->
<!--    \item -->
<!--    In our example, responses were recorded to each stimulus item at different learning trials, so at different learning blocks, so, also we can say that learning block was manipulated within-items} and we might want to see how the slope of the block effect was different for responses to different stimuli -->
<!--     -->
<!--     -->
<!-- % -- random slopes of what? -->
<!-- % -- barr et al 2013 p275 -- -->
<!-- So the first question to ask oneself when trying to specify a linear mixed-effects models with random effects is (Barr et al., 2013; p. 275), in their words:  -->
<!-- \begin{quotation} -->
<!-- If a factor is between-unit, then a random intercept is usually sufficient.  -->
<!-- If a factor is within-unit and there are multiple observations per treatment level per unit, then you need a by-unit random slope for that factor.  -->
<!-- The only exception to this rule is when you only have a single observation for every treatment level of every unit; in this case, the random slope variance would be completely confounded with trial-level error. It follows that a model with a random slope would be unidentifiable, and so a random intercept would be sufficient to meet the conditional independence assumption. -->
<!-- \end{quotation} -->
<!-- % -- so eg SOA is between subjects because we have multiple different subjects in each level of the factor -->
<!-- % -- while frequency is within subjects because we have multiple observations per treatment level per subject -- we can think of multiple words at each frequency level -->
<!-- % -- we could have one observation per stimulus per level per subject -- so a high frequency word for person1, a low frequency word for person2 -- we cannot then distinguish differences between person from differences due to frequency -->
<!-- % -- barr et al 2013 p260 -- -->
<!-- % At this point, we might wish to go further and consider other models. For instance, we have considered a by-sub- ject random slope; for consistency, why not also consider a model with a by-item random slope, I1i? A little reflection reveals that a by-item random slope does not make sense for this design. Words are nested within word types—no word can be both type A and type B—so it is not sensible to ask whether words vary in their sensitivity to word type. No sample from this experiment could possibly give us the information needed to estimate random slope variance and random slope/intercept covariance parameters for such a model. A model like this is unidentifiable for the data it is applied to: there are (infinitely) many different values we could choose for its parameters which would describe the data equally well.4 Experiments with a within-item manipulation, such as a priming experiment in which target words are held constant across conditions but the prime word is varied, would call for by-item random slopes, but not the current experiment. -->
<!-- % The above point also extends to designs where one independent variable is manipulated within- and another variable between- subjects (respectively items). In case of between-subject manipulations, the levels of the subject variable are nested within the levels of the experimental treatment variable (i.e. each subject belongs to one and only one of the experimental treatment groups), meaning that subject and treatment cannot interact—a model with a by-subject random slope term would be unidentifiable. In general, within-unit treatments require both the by-unit intercepts and slopes in the random effects specification, whereas between-unit treatments require only the by-unit random intercepts. -->
<!-- % ... -->
<!-- % to form a cluster in the sample, it is necessary to have more than one observation for a given unit; otherwise, the clustering effect cannot be distin- guished from residual error.5 If we only elicit one observa- tion per subject/item combination, we are unable to estimate this source of variability, and the model containing this random effect becomes unidentifiable. Had we used a design with repeated exposures to the same items for a gi- ven subject, the same model would be identifiable, and in fact we would need to include that term to avoid violating the conditional independence of our observations given sub- ject and item effects. -->
<!-- % -- so in short: -->
<!-- % -- a factor is within-unit if there are multiple observations per treatment level per unit -- whether subjects or items -->
<!-- % -- an example of a within-items manipulation is priming where you might see the same items both under primed and nonprimed conditions -->
<p>In deciding what random effects we should specify, we need to think about what response data we have recorded, for each of the experimental variables, given the study design. This is because if we want to specify a random effect of participants (or stimulus words) on the slope of an experimental condition then we need to have data, for each person, on their responses under all levels of the condition. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> If we want to estimate the effect of the experimental manipulation of learning condition, for example, the impact of the presence of orthography, for a person, we need to have data for both levels of the condition (orthography absent and orthography present) for that person. If you think about it, we cannot estimate the slope of the effect of the presence of orthography without response data recorded under both the orthography absent condition and the orthography present condition. If we can estimate the slope of the effect then} we can estimate how the slope of the effect deviates between participants. <!-- That is, if we can estimate the slope of the effect for any one participant then} we can estimate the random effect of participants on the slope of the effect. --> <!-- If we do not have response data, for any participant under all levels of a condition (for example, because different people are tested under different learning conditions) then we cannot estimate the slope of the impact of that manipulation for any one participant, and therefore we cannot estimate how the slope varies between participants. --></p>
<p>We can spell out how the experimental conditions were manipulated for the example study, as follows. (Writing out this kind of account, for yourself, will be helpful perhaps when you are planning an analysis and have to work out what the random effects could be.)</p>
<!--  -->
<!-- \item -->
<!-- The effect of Orthography was manipulated within} participants and within} stimulus words. This is because the presence of orthography (orthography absent versus orthography present) was manipulated so that we have data about test responses to each word under both Orthography conditions, and data about responses from each child under both conditions. Because we wanted to avoid presenting the same word under both conditions for each child, we used what is called counterbalancing. This means we split the participant sample into two groups. For each child, eight (out of 16) of the words were taught with orthography present and the other eight (out of 16) with orthography absent. This means each child experienced learning under both conditions, and we have response data for each word under both conditions, but we avoid confounding condition differences with repetition of word presentation. -->
<!-- \item -->
<!-- The effect of Instructions was manipulated between} participants. This is because Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not. Participants completing explicit and incidental conditions (n = 20 in explicit condition; 21 in incidental condition) were matched in pairs for vocabulary knowledge and word reading ability, and then matched as closely as possible for gender, age and nonverbal reasoning. -->
<!-- \item -->
<!-- We can say that the effects of Orthography and of Instructions are both manipulated within} words. Items were counterbalanced across instruction and orthography conditions, with all words appearing in both orthography conditions for approximately the same number of children within the explicit and incidental groups. -->
<!-- \item -->
<!-- The effect of spelling-sound consistency varies between} words because different words have different consistency values but the effect of consistency varies within} participants because we have response data for each participant for their responses to words of different levels of consistency. -->
<!-- \item -->
<!-- We recorded responses for all participants and all words so we can say that the effect of Time (test time 1 versus time 2) can also be understood to vary within} both participants and words. This means that, for each person's response to each word on which they are tested, we have response data recorded at both test times. -->
<!--  -->
<ul>
<li>The effect of Orthography was manipulated within} participants and within} stimulus words. This is because the presence of orthography (orthography absent versus orthography present) was manipulated so that we have data about test responses to each word under both Orthography conditions, and data about responses from each child under both conditions.</li>
<li>The effect of Instructions was manipulated between} participants. This is because Instructions (incidental vs.&nbsp;explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.</li>
<li>We can say that the effects of Orthography and of Instructions are both manipulated within} words. Items were counterbalanced across instruction and orthography conditions, with all words appearing in both orthography conditions for approximately the same number of children within the explicit and incidental groups.</li>
<li>The effect of spelling-sound consistency varies between} words because different words have different consistency values but the effect of consistency varies within} participants because we have response data for each participant for their responses to words of different levels of consistency.</li>
<li>We recorded responses for all participants and all words so we can say that the effect of Time (test time 1 versus time 2) can also be understood to vary within} both participants and words. This means that, for each person’s response to each word on which they are tested, we have response data recorded at both test times.</li>
</ul>
<p>These considerations suggests that we should specify a model with the random effects:</p>
<ul>
<li>The random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.</li>
<li>The random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.</li>
</ul>
<p>This is simple to do using the code shown following.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>long.orth.max.glmer <span class="ot">&lt;-</span> <span class="fu">glmer</span>(Score <span class="sc">~</span> </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                           Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span> </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>                           Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>                           Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>                           (Time <span class="sc">+</span> Orthography <span class="sc">+</span> zConsistency_H <span class="sc">+</span> <span class="dv">1</span> <span class="sc">|</span> Participant) <span class="sc">+</span> </span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>                           (Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> <span class="dv">1</span> <span class="sc">|</span>Word),</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>                         </span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>                         <span class="at">family =</span> <span class="st">"binomial"</span>,</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">glmerControl</span>(<span class="at">optimizer=</span><span class="st">"bobyqa"</span>, <span class="at">optCtrl=</span><span class="fu">list</span>(<span class="at">maxfun=</span><span class="fl">2e5</span>)),</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>                         </span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>                         <span class="at">data =</span> long.orth)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth.max.glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Where we have the critical terms:</p>
<ul>
<li><code>(Time + Orthography + zConsistency_H + 1 | Participant)</code> to account for the random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.</li>
<li><code>(Time + Orthography + Instructions + 1 |Word)</code> the random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.</li>
</ul>
<p>If you run this code, however, you will see that you get warnings along with your estimates.</p>
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>boundary (singular) fit: see help('isSingular')</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: Score ~ Time + Orthography + Instructions + zConsistency_H +  
    Orthography:Instructions + Orthography:zConsistency_H + (Time +  
    Orthography + zConsistency_H + 1 | Participant) + (Time +  
    Orthography + Instructions + 1 | Word)
   Data: long.orth
Control: glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e+05))

     AIC      BIC   logLik deviance df.resid 
  1053.6   1192.4   -499.8    999.6     1236 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.1014 -0.4027 -0.1723  0.2037  7.0331 

Random effects:
 Groups      Name           Variance Std.Dev. Corr             
 Participant (Intercept)    2.043126 1.42938                   
             Time2          0.005675 0.07533   0.62            
             Orthography2   0.079980 0.28281   0.78 -0.01      
             zConsistency_H 0.065576 0.25608   0.49  0.99 -0.16
 Word        (Intercept)    2.793447 1.67136                   
             Time2          0.046736 0.21618   0.14            
             Orthography2   0.093740 0.30617  -0.68 -0.81      
             Instructions1  0.212706 0.46120  -0.74 -0.05  0.38
Number of obs: 1263, groups:  Participant, 41; Word, 16

Fixed effects:
                            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -2.10099    0.49588  -4.237 2.27e-05 ***
Time2                        0.02077    0.12285   0.169 0.865741    
Orthography2                 0.52480    0.15496   3.387 0.000708 ***
Instructions1                0.24467    0.27281   0.897 0.369801    
zConsistency_H              -0.67818    0.36310  -1.868 0.061802 .  
Orthography2:Instructions1  -0.05133    0.10004  -0.513 0.607908    
Orthography2:zConsistency_H  0.05850    0.11634   0.503 0.615064    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1
Time2        0.097                                   
Orthogrphy2 -0.280 -0.187                            
Instructns1 -0.278  0.023  0.109                     
zCnsstncy_H  0.062  0.005 -0.064  0.120              
Orthgrp2:I1  0.024  0.005 -0.071  0.212 -0.065       
Orthgr2:C_H -0.062  0.049  0.246 -0.031 -0.440  0.001
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see help('isSingular')</code></pre>
</div>
</div>
<p>Notice, especially, the warning:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boundary</span> (singular) fit<span class="sc">:</span> see ?isSingular</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you put the warning message text into a search engine, then you will get directed to a variety of discussions about what they mean and what you should do about them.</p>
<p>A highly instructive blog post by (I think) Ben Bolker provides some very useful <a href="https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html">advice</a></p>
<p>Where we see this advice:</p>
<blockquote class="blockquote">
<p><strong>Check singularity</strong></p>
<p>If the fit is singular or near-singular, there might be a higher chance of a false positive (we’re not necessarily screening out gradient and Hessian checking on singular directions properly); a higher chance that the model has actually misconverged (because the optimization problem is difficult on the boundary); and a reasonable argument that the random effects model should be simplified}.</p>
<p>The definition of singularity is that some of the constrained parameters of the random effects theta parameters are on the boundary (equal to zero, or very very close to zero …)</p>
</blockquote>
<p>(Emphases added.)</p>
<p>I am going to take his advice and simplify the random effects part of the model. We know that the random intercepts model converges fine and now we know that the maximal model does not. Thus, our task is now to identify a model that includes random effects of participants or items on slopes and still converges without warnings.</p>
</section>
<section id="sec-glmm-comparing-models" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="sec-glmm-comparing-models"><span class="header-section-number">6.9.1</span> Examine the utility of random effects by comparing models with the same fixed effects but varying random effects</h3>
<p>I am just going to assume we need both random effects of subjects and of items on intercepts so I focus on random slopes} here. (This assumption may not always be true but is often useful.)</p>
<p>We can fit a series of models as follows. Note that I will not show the results for every model, to save space, but you should run the code to see what happens. Look out for convergence or singularity warnings, where they appear.</p>
<section id="sec-glmm-random-effects-subjects-items-intercepts" class="level4" data-number="6.9.1.1">
<h4 data-number="6.9.1.1" class="anchored" data-anchor-id="sec-glmm-random-effects-subjects-items-intercepts"><span class="header-section-number">6.9.1.1</span> Random effects of subjects and stimulus items on intercepts</h4>
<p>In the first model, we have just random effects of participants or items on intercepts. This is where we started.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>long.orth.min.glmer <span class="ot">&lt;-</span> <span class="fu">glmer</span>(Score <span class="sc">~</span> </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>                               Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span> </span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>                               Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>                               Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>                               (<span class="dv">1</span> <span class="sc">|</span> Participant) <span class="sc">+</span> </span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>                               (<span class="dv">1</span> <span class="sc">|</span> Word),</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>                             <span class="at">family =</span> <span class="st">"binomial"</span>, </span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">glmerControl</span>(<span class="at">optimizer=</span><span class="st">"bobyqa"</span>, <span class="at">optCtrl=</span><span class="fu">list</span>(<span class="at">maxfun=</span><span class="fl">2e5</span>)),</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>                             <span class="at">data =</span> long.orth)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth.min.glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We saw that the model converged, and we looked at the results previously.</p>
<p>What next? A simple approach we can take is to see if we can add each fixed effect to be included in our random effects terms, one effect at a time.</p>
</section>
<section id="sec-glmm-random-effects-subjects-items-slopes" class="level4" data-number="6.9.1.2">
<h4 data-number="6.9.1.2" class="anchored" data-anchor-id="sec-glmm-random-effects-subjects-items-slopes"><span class="header-section-number">6.9.1.2</span> Random effects of subjects and stimulus words on the slope of the Orthography effect</h4>
<p>In our second model, we change the random effects terms so that we can account for the random effects of participants and of items on intercepts as well as on the slopes of the Orthography effect. (The Orthography effect is both within-subjects and within-items.)</p>
<div class="cell" data-information="false" data-hash="04-glmm_cache/html/orth-2-glmer_e5a54126149ecba17b83e3e04a3cdb63">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>long.orth.<span class="fl">2.</span>glmer <span class="ot">&lt;-</span> <span class="fu">glmer</span>(Score <span class="sc">~</span> </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>                             Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span> </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Participant) <span class="sc">+</span> </span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Word),</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>                           <span class="at">family =</span> <span class="st">"binomial"</span>, </span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">glmerControl</span>(<span class="at">optimizer=</span><span class="st">"bobyqa"</span>, <span class="at">optCtrl=</span><span class="fu">list</span>(<span class="at">maxfun=</span><span class="fl">2e5</span>)),</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>                           <span class="at">data =</span> long.orth)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth.<span class="fl">2.</span>glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This model converges without warnings.</p>
<section id="code-tip-3" class="level5" data-number="6.9.1.2.1">
<h5 data-number="6.9.1.2.1" class="anchored" data-anchor-id="code-tip-3"><span class="header-section-number">6.9.1.2.1</span> Code tip</h5>
<p>Notice that we specify that we <code>||</code> do not want random covariances; we are keeping things simple in each step.</p>
<p>Note the use of <code>dummy()</code> inside the random effects terms. The ‘dummy’ is a mis-leading name; we are not talking about dummy coding (as above). Here, the <code>dummy()</code> stops R from mis-interpreting the requirement to estimate the effect of the differences between category levels, within random effects.</p>
<!-- , when you are using the `||: notation. -->
<!-- -- maybe need to review notes from when worked to assist SSSR workshop -->
<!-- https://rpubs.com/Reinhold/22193 -->
<!-- https://rpubs.com/Reinhold/391027 -->
<p>The reason is explained <a href="https://rdrr.io/cran/lme4/man/dummy.html">here</a>.</p>
<!-- -- see also from a discussion thread with alan garnham 2018 emails, ST&D R workshop: -->
<!-- https://github.com/lme4/lme4/issues/182 -->
<p>You can see what impact it has by specifying, instead, the naked random effect:</p>
<p><code>(Orthography + 1 || Participant)</code>.</p>
</section>
</section>
<section id="sec-glmm-random-effects-subjects-items-slopes-2" class="level4" data-number="6.9.1.3">
<h4 data-number="6.9.1.3" class="anchored" data-anchor-id="sec-glmm-random-effects-subjects-items-slopes-2"><span class="header-section-number">6.9.1.3</span> Random effects of subjects and stimulus words on the slope of the Instructions effect</h4>
<p>Next we can add `Instructions: to take into account random differences between words in the slope of this effect. We show the results for this model as they are instructive.</p>
<div class="cell" data-hash="04-glmm_cache/html/orth-3-glmer_a0209cd36a1c2aa0fbb24a279aafc9b1">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>long.orth.<span class="fl">3.</span>glmer <span class="ot">&lt;-</span> <span class="fu">glmer</span>(Score <span class="sc">~</span> </span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>                             Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span> </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Participant) <span class="sc">+</span> </span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="fu">dummy</span>(Instructions) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Word),</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>                           <span class="at">family =</span> <span class="st">"binomial"</span>, </span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">glmerControl</span>(<span class="at">optimizer=</span><span class="st">"bobyqa"</span>, <span class="at">optCtrl=</span><span class="fu">list</span>(<span class="at">maxfun=</span><span class="fl">2e5</span>)),</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>                           <span class="at">data =</span> long.orth)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth.<span class="fl">3.</span>glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: Score ~ Time + Orthography + Instructions + zConsistency_H +  
    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  
    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  
    1 || Word)
   Data: long.orth
Control: glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e+05))

     AIC      BIC   logLik deviance df.resid 
  1036.5   1098.2   -506.2   1012.5     1251 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9951 -0.4068 -0.1920  0.1838  5.9308 

Random effects:
 Groups        Name                Variance Std.Dev.
 Participant   (Intercept)         1.64393  1.2822  
 Participant.1 dummy(Orthography)  0.55604  0.7457  
 Word          (Intercept)         1.94313  1.3940  
 Word.1        dummy(Orthography)  0.01608  0.1268  
 Word.2        dummy(Instructions) 0.86694  0.9311  
Number of obs: 1263, groups:  Participant, 41; Word, 16

Fixed effects:
                              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -1.9621258  0.4400570  -4.459 8.24e-06 ***
Time2                        0.0507347  0.0843236   0.602  0.54740    
Orthography2                 0.4263703  0.1114243   3.827  0.00013 ***
Instructions1                0.1907424  0.2632579   0.725  0.46873    
zConsistency_H              -0.6270892  0.3669873  -1.709  0.08750 .  
Orthography2:Instructions1  -0.0265727  0.1048392  -0.253  0.79991    
Orthography2:zConsistency_H -0.0006303  0.0878824  -0.007  0.99428    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1
Time2        0.007                                   
Orthogrphy2  0.028  0.008                            
Instructns1 -0.127  0.022  0.019                     
zCnsstncy_H  0.017 -0.002 -0.026  0.011              
Orthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       
Orthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018</code></pre>
</div>
</div>
<p>This model also converges without warnings.</p>
<p>Take a look at the random effects summary. You can see:</p>
<ul>
<li><code>Participant (Intercept) 1.64393</code> estimated variance due to random effect of participants on intercepts</li>
<li><code>Participant.1 dummy(Orthography) 0.55604</code> variance due to random effect of participants on the slope of the Orthography effect</li>
<li><code>Word (Intercept) 1.94314</code> variance due to random effect of words on intercepts</li>
<li><code>Word.1 dummy(Orthography) 0.01607</code> variance due to random effect of words on the slope of the Orthography effect</li>
<li><code>Word.2 dummy(Instructions) 0.86694</code> variance due to random effect of words on the slope of the Instructions effect</li>
</ul>
<p>We do <em>not</em> see correlations (random effects covariances) because we use the <code>||</code> notation to stop them being estimated. We want to stop them being estimated because we want to see what we gain from adding just the requirement, first, to estimate the variance associated with random effects of participants or words on the slopes of the experimental variables.</p>
<p>Also, we can suspect that adding the requirement to estimate covariances will blow the model up for two reasons. The maximal model, including random slopes variances <em>and</em> covariances clearly did not converge. Secondly, at least two of the correlations listed in the random effects, for the maximal model, were pretty extreme with `Corr: <span class="math inline">\(=-0.01\)</span> and <span class="math inline">\(=0.99\)</span>; such extreme values (<span class="math inline">\(r \sim \pm 1\)</span>) are bad signs; see the discussion in @ref(bad-signs).</p>
</section>
<section id="sec-glmm-random-effects-add" class="level4" data-number="6.9.1.4">
<h4 data-number="6.9.1.4" class="anchored" data-anchor-id="sec-glmm-random-effects-add"><span class="header-section-number">6.9.1.4</span> Adding effects a bit at a time</h4>
<p>In the following models, because we can see that we can get a model to converge with random effects of participants or items on Orthography, and random effect of participants on Instructions, I am going to keep these random effects in the model. I will check if adding further effects is OK too, in terms of successful convergence. I am going to treat all the following models as variations on a theme, the theme being: can we add anything else to:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Participant) <span class="sc">+</span> </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="fu">dummy</span>(Instructions) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Word),</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-glmm-random-effects-subjects-items-slopes-3" class="level4" data-number="6.9.1.5">
<h4 data-number="6.9.1.5" class="anchored" data-anchor-id="sec-glmm-random-effects-subjects-items-slopes-3"><span class="header-section-number">6.9.1.5</span> Random effects of subjects on the slope of the consistency effect</h4>
<p>Next we see if we can add <code>zConsistency_H</code>.</p>
<div class="cell" data-hash="04-glmm_cache/html/orth-4-a-glmer_00d41764ff87f4152dddb3eb96de69be">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>long.orth.<span class="fl">4.</span>a.glmer <span class="ot">&lt;-</span> <span class="fu">glmer</span>(Score <span class="sc">~</span> </span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>                             Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span> </span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> zConsistency_H <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Participant) <span class="sc">+</span> </span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="fu">dummy</span>(Instructions) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Word),</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>                           <span class="at">family =</span> <span class="st">"binomial"</span>, </span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">glmerControl</span>(<span class="at">optimizer=</span><span class="st">"bobyqa"</span>, <span class="at">optCtrl=</span><span class="fu">list</span>(<span class="at">maxfun=</span><span class="fl">2e5</span>)),</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>                           <span class="at">data =</span> long.orth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>boundary (singular) fit: see help('isSingular')</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth.<span class="fl">4.</span>a.glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: Score ~ Time + Orthography + Instructions + zConsistency_H +  
    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  
    zConsistency_H + 1 || Participant) + (dummy(Orthography) +  
    dummy(Instructions) + 1 || Word)
   Data: long.orth
Control: glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e+05))

     AIC      BIC   logLik deviance df.resid 
  1038.5   1105.3   -506.2   1012.5     1250 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9952 -0.4068 -0.1920  0.1838  5.9309 

Random effects:
 Groups        Name                Variance  Std.Dev. 
 Participant   (Intercept)         1.644e+00 1.282e+00
 Participant.1 dummy(Orthography)  5.560e-01 7.456e-01
 Participant.2 zConsistency_H      1.259e-10 1.122e-05
 Word          (Intercept)         1.943e+00 1.394e+00
 Word.1        dummy(Orthography)  1.604e-02 1.266e-01
 Word.2        dummy(Instructions) 8.669e-01 9.311e-01
Number of obs: 1263, groups:  Participant, 41; Word, 16

Fixed effects:
                              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -1.9621298  0.4400610  -4.459 8.24e-06 ***
Time2                        0.0507346  0.0843237   0.602  0.54740    
Orthography2                 0.4263731  0.1114190   3.827  0.00013 ***
Instructions1                0.1907320  0.2632610   0.724  0.46876    
zConsistency_H              -0.6270931  0.3669852  -1.709  0.08749 .  
Orthography2:Instructions1  -0.0265706  0.1048369  -0.253  0.79992    
Orthography2:zConsistency_H -0.0006353  0.0878784  -0.007  0.99423    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1
Time2        0.007                                   
Orthogrphy2  0.028  0.008                            
Instructns1 -0.127  0.022  0.019                     
zCnsstncy_H  0.017 -0.002 -0.026  0.011              
Orthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       
Orthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see help('isSingular')</code></pre>
</div>
</div>
<p>We see two useful pieces of information when we run this model:</p>
<ul>
<li>We get the warning `boundary (singular) fit: see ?isSingular: that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data</li>
<li>Note, also, we see the random effects variance estimate <code>Participant.2 zConsistency_H 1.259e-10</code>.</li>
</ul>
<p>These two things are possibly connected: the singularity warning; and the estimated variance of <span class="math inline">\(1.259e-10\)</span> (i.e.&nbsp;a very very small number) associated with the random effect of participants on the slope of the <code>zConsistency_H</code>. We can expect that the model fitting algorithm is going to have difficulty estimating nothing, or something close to nothing: here, the very very small variance associated with the between-participant differences in the slope of the non-significant effect of word spelling-sound consistency on response accuracy.</p>
</section>
<section id="sec-glmm-random-effects-subjects-items-slopes-4" class="level4" data-number="6.9.1.6">
<h4 data-number="6.9.1.6" class="anchored" data-anchor-id="sec-glmm-random-effects-subjects-items-slopes-4"><span class="header-section-number">6.9.1.6</span> Random effects of subjects and stimulus words on the slope of the time effect</h4>
<p>What about the random effects of participants or of words on the slope of the effect of Time?</p>
<div class="cell" data-hash="04-glmm_cache/html/orth-4-b-glmer_4f4edcceda6e6f46d0346a63df5006f0">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>long.orth.<span class="fl">4.</span>b.glmer <span class="ot">&lt;-</span> <span class="fu">glmer</span>(Score <span class="sc">~</span> </span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>                             Time <span class="sc">+</span> Orthography <span class="sc">+</span> Instructions <span class="sc">+</span> zConsistency_H <span class="sc">+</span> </span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>Instructions <span class="sc">+</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>                             Orthography<span class="sc">:</span>zConsistency_H <span class="sc">+</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="fu">dummy</span>(Time) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Participant) <span class="sc">+</span> </span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>                             (<span class="fu">dummy</span>(Orthography) <span class="sc">+</span> <span class="fu">dummy</span>(Instructions) <span class="sc">+</span> <span class="fu">dummy</span>(Time) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">||</span> Word),</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>                           <span class="at">family =</span> <span class="st">"binomial"</span>, </span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">glmerControl</span>(<span class="at">optimizer=</span><span class="st">"bobyqa"</span>, <span class="at">optCtrl=</span><span class="fu">list</span>(<span class="at">maxfun=</span><span class="fl">2e5</span>)),</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>                           </span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>                           <span class="at">data =</span> long.orth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>boundary (singular) fit: see help('isSingular')</code></pre>
</div>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(long.orth.<span class="fl">4.</span>b.glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: Score ~ Time + Orthography + Instructions + zConsistency_H +  
    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  
    dummy(Time) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  
    dummy(Time) + 1 || Word)
   Data: long.orth
Control: glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e+05))

     AIC      BIC   logLik deviance df.resid 
  1040.5   1112.5   -506.2   1012.5     1249 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9952 -0.4068 -0.1920  0.1838  5.9309 

Random effects:
 Groups        Name                Variance  Std.Dev. 
 Participant   (Intercept)         1.644e+00 1.2821915
 Participant.1 dummy(Orthography)  5.560e-01 0.7456312
 Participant.2 dummy(Time)         0.000e+00 0.0000000
 Word          (Intercept)         1.943e+00 1.3939636
 Word.1        dummy(Orthography)  1.604e-02 0.1266478
 Word.2        dummy(Instructions) 8.669e-01 0.9310620
 Word.3        dummy(Time)         2.161e-07 0.0004648
Number of obs: 1263, groups:  Participant, 41; Word, 16

Fixed effects:
                              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -1.9621302  0.4400578  -4.459 8.24e-06 ***
Time2                        0.0507347  0.0843253   0.602  0.54740    
Orthography2                 0.4263731  0.1114192   3.827  0.00013 ***
Instructions1                0.1907329  0.2632605   0.725  0.46876    
zConsistency_H              -0.6270910  0.3669905  -1.709  0.08750 .  
Orthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    
Orthography2:zConsistency_H -0.0006351  0.0878784  -0.007  0.99423    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1
Time2        0.007                                   
Orthogrphy2  0.028  0.008                            
Instructns1 -0.127  0.022  0.019                     
zCnsstncy_H  0.017 -0.002 -0.026  0.011              
Orthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       
Orthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see help('isSingular')</code></pre>
</div>
</div>
<p>Nope.</p>
<p>We again see two useful pieces of information when we run this model:</p>
<ul>
<li>We get the warning <code>boundary (singular) fit</code> see <code>?isSingular</code> that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data</li>
<li>Note, also, we see the random effects variance estimate <code>Participant.2 dummy(Time) 0.000e+00</code>.</li>
<li>And we see the variance estimate <code>Word.3 dummy(Time) 2.161e-07</code>.</li>
</ul>
<p>Again, we can surmise that a mixed-effects model will get into trouble – and we will see convergence warnings – where we have a fixed effect with little impact (like, here, Time, or, before consistency) and we are asking for the estimation of variance associated with random differences in slopes where there may be, in fact, little random variation. Possibly, these two things are connected too: we are perhaps unlikely to see random differences in the slope of the effect of an experimental variable if the effect is at or near zero. Possibly, we may see the effect of an experimental variable which is very very consistent. Think back to Week 16 and the effect associated with the relation between maths and physics scores where there seemed to be little variation between classes in the slope representing the relation.</p>
</section>
</section>
<section id="sec-glmm-bad-signs" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="sec-glmm-bad-signs"><span class="header-section-number">6.9.2</span> Bad signs</h3>
<p>We can see that a model has difficulty if we see things like:</p>
<ul>
<li>Convergence warnings, obviously</li>
<li>Very very small random effects variances</li>
<li>Extreme random effects correlations of <span class="math inline">\(\pm 1.00\)</span></li>
</ul>
<!--  -->
<!-- \item -->
<!-- The variance estimate associated with random effects of items on the slope of the fixed effect of block is very very small: $4.650e-05$ for the effect of target objects `(cblock + 1 | targetobject):; and $3.912e-06$ for the effect of target actions `(cblock + 1 | targetaction): -->
<!-- \item -->
<!-- Consider how small are the deviations due to random differences between items when the spread of such differences -- given by the variance estimates -- are numbers as small as about a millionth $3.912e-06 = 0.000003912$ (if you remember your scientific notation) -->
<!-- \item -->
<!-- We are looking then at an effect of blocks that does not vary much, if at all, between responses to different stimulus items -->
<!-- \item -->
<!-- In comparison, the effect on subjects on the slope of the effect of blocks is 6,500 times larger, associated with a variance of $2.527e-02 = 0.02527$ -->
<!-- \item -->
<!-- We can conclude that specifying a model that adds complexity in terms of the random effect of items on the slope of the blocks effect is not justified -->
<!-- \item -->
<!-- A further warning sign can be seen in random effects covariances -- see the values listed in the columns under `Corr: -->
<!-- \item -->
<!-- We can see extreme correlations of $1.00$ -->
<!-- \item -->
<!-- Correlations as high as these strongly suggest that the model has difficulty capturing the associated random effects (like the effect of items on the slopes of the block effect) because they are not really present in the data -->
<!--  -->
<p>If we see a warning that the model fitting algorithm nearly failed to converge: <code>boundary (singular) fit: see ?isSingular</code> or failed to converge then this tells us that, given the data, the mathematical engine (optimizer) underlying the `lmer(): function model fitting got into trouble because, in short, it was trying to find estimates for effects that were close to not being there at all.</p>
<p>If the variances for the random effects of participants or stimulus items on the slopes of an experimental variable are very small this suggests that the level of complexity in the model cannot really be justified or that the model will have difficulty estimating it. Extreme correlations (near 0 or 1) between random effects on intercepts and on slopes of fixed effects suggest the level of complexity in the model cannot really be justified (see also the discussion in Bates et al., 2015; Matuschek et al., 2017).</p>
</section>
<section id="sec-glmm-comparing0-models-2" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="sec-glmm-comparing0-models-2"><span class="header-section-number">6.9.3</span> Comparison of models varying in random effects</h3>
<p>There is no point comparing the models that do not converge, so we focus on those that do.</p>
<p>Does the addition of random slopes improve model fit? We can compare the model in pairs, as follows, to test whether each addition in model complexity improves model fit. We run the code for the model comparisons as follows.</p>
<p>First, we compare the model <code>long.orth.min.glmer</code> (just random intercepts) with <code>long.orth.2.glmer</code> to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Orthography effect improves model fit to data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(long.orth.min.glmer, long.orth.<span class="fl">2.</span>glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data: long.orth
Models:
long.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)
long.orth.2.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + 1 || Word)
                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)
long.orth.min.glmer    9 1040.4 1086.7 -511.20   1022.4                     
long.orth.2.glmer     11 1041.0 1097.6 -509.51   1019.0 3.3909  2     0.1835</code></pre>
</div>
</div>
<p>Second, we compare the model <code>long.orth.min.glmer</code> (just random intercepts) with <code>long.orth.3.glmer</code> to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Instructions effect improves model fit to data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(long.orth.min.glmer, long.orth.<span class="fl">3.</span>glmer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data: long.orth
Models:
long.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)
long.orth.3.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) + 1 || Word)
                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  
long.orth.min.glmer    9 1040.4 1086.7 -511.20   1022.4                       
long.orth.3.glmer     12 1036.5 1098.2 -506.24   1012.5 9.9115  3    0.01933 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>I think this justifies reporting the model `long.orth.3.glmer:</p>
<p>If you look at the outputs, the addition of the random effects of participants and words on the slope of the effect of Orthography cannot be justified (<span class="math inline">\(\chi^2 = 3.3909, 2 df, p = .1835\)</span>) by improved model fit, in comparison to a model with the random effects of participants and words on intercepts.</p>
<p>The model comparison summary indicates that the addition of a random effect of words on the slope of the Instructions effect is justified by significantly improved model fit to data (<span class="math inline">\(\chi^2 = 9.9115, 3 df, p = 0.01933\)</span>).</p>
<p>Notice I take a bit of a short-cut here, by adding both random effects for Orthography and Instructions. Logically, if adding the random effect for Orthography does not improve model fit but adding the random effects for both random effects for Orthography and Instructions does then it is the addition of Instructions that is doing the work. However, I should prefer to see as complex a random effects structure as possible, provided a model converges.</p>
<p>While adding the random effect of Orthography does not improve model fit significantly, you see researchers allowing a generous p-value threshold for inclusion of terms (i.e.&nbsp;it is ok to add variables up ton where <span class="math inline">\(.p &lt; .2\)</span>). Matuschek et al.&nbsp;(2017) argue that when we are engaged in model selection – here, this is what we are doing because we are trying to figure out what model (with what random effects) we should use – then we should resist the reflex to choose the model that seems justified because the LRT <span class="math inline">\(\chi^2, p &lt; .05\)</span>. The <span class="math inline">\(\chi^2\)</span> alpha level cannot be interpreted as the expected model selection Type I error rate (in Null Hypothesis Significance Test terms) but rather as the relative weight of model complexity and goodness-of-fit (look back at the discussion of model comparison in the previous chapter). In this sense, setting a threshold such that we include an effect only if <span class="math inline">\(\chi^2, p &lt; .05\)</span> will always tend to penalize model complexity, and tend therefore to lead us to choose simpler (perhaps too simple) models.</p>
</section>
<section id="sec-glmm-addressing-convergence" class="level3" data-number="6.9.4">
<h3 data-number="6.9.4" class="anchored" data-anchor-id="sec-glmm-addressing-convergence"><span class="header-section-number">6.9.4</span> Addressing convergence problems</h3>
<p>Sometimes (g)lmer() has difficulty finding estimates for effects in a model given a dataset. If it encounters problems, the problems are expressed as warnings about convergence failures. Convergence failures typically arise when the model is too complicated for the data (see the discussion in Bates et al., 2015; Eager &amp; Roy, 2017; Matuschek et al., 2017; Meteyard &amp; Davies, 2020). As we have seen, problems can occur if you are trying to estimate (or predict) random effects terms that are very small – that do not really explain much variance in performance. Problems can also occur if variables have very large or very small ranges.</p>
<p>We can detect and address these problems in a number of ways:</p>
<ol type="1">
<li>Sometimes convergence problems can be fixed by switching the optimizer used to fit the model – we can do this by adding the argument: <code>glmerControl(optimize = "bobyqa")</code> to the <code>glmer()</code> function call, as I did for the class example. Switching optimizers is a quick solution to a common problem: models can fail to converge for a number of different reasons. In short, there may not be enough data for the model fitting process to settle on appropriate estimates for fixed or random effects.</li>
<li>Sometimes, a warning message advises us to consider rescaling the continuous numeric predictor variable. For this reason, and others, I usually standardize numeric predictor variables, as a default, before the analysis.</li>
<li>Sometimes, the warnings tell us that we need to simplify the random effects part of the model. We can simplify the random effects structure of a mixed-effects model in a number of ways: As we examine the estimates resulting from a model fit we can consider whether the variance and covariance terms are small or large. Ultimately, I decided that the effects of subjects and items on intercepts was important, as was, to some extent, the effect of words on the slope of the effect of Instructions.</li>
</ol>
<!-- By fitting a series of models of increasing complexity, and then comparing the model in a series of paired model comparisons, we find evidence suggesting that the most complex model, including both main effects of learning trial block and experimental condition, as well as the effect of the interaction, fits the data better than any simpler model. -->
<p>The approach we have progressed through is widely used (see discussions in Baayen et al., 2008; Barr, et al., 2013; Luke, 2017; Matuschek et al., 2017; Meteyard &amp; Davies, 2019). <!-- You might take this approach, then, if you are investigating whether effects that interest you, for theoretical or practical reasons, are or are not helpful in your research when you are seeking to specify an account of the factors that result in the data you observed. --></p>
<p>See, especially, the troubleshooting guide by Ben Bolker <a href="https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html">here</a>.</p>
<section id="exercises" class="level4" data-number="6.9.4.1">
<h4 data-number="6.9.4.1" class="anchored" data-anchor-id="exercises"><span class="header-section-number">6.9.4.1</span> Exercises</h4>
<p>To demonstrate the impact of these adjustments, you could refit the models discussed in the foregoing:</p>
<ul>
<li>Without using the standardized consistency variable as a predictor</li>
<li>Without using the modification to the model fitting code, i.e.&nbsp;deleting the line that includes <code>glmerControl(optimizer = "bobyqa")</code></li>
<li>Making the last most complex model more complex by adding further random effects of subjects or items on the slopes of both main effects and the interaction</li>
</ul>
</section>
<section id="sec-glmm-advice-summary" class="level4" data-number="6.9.4.2">
<h4 data-number="6.9.4.2" class="anchored" data-anchor-id="sec-glmm-advice-summary"><span class="header-section-number">6.9.4.2</span> Summary advice</h4>
<p>My advice, then, is to consider whether random effects should be included in a model based on</p>
<ul>
<li>Theoretical reasons, in terms of what your understanding of a study design allows and requires, with respect to random differences between groups (classes, participants, stimuli etc.) or stimuli;</li>
<li>Model convergence, as when models do or do not converge;</li>
<li>Over a series of model comparisons, an evaluation of whether model fit is improved by the inclusion of the random effect.</li>
</ul>
<p>This sounds like it involves work, judgment, and a process. It also sounds like people may disagree on the judgment or the process so that you shall have to share data and code, to enable others to check if the results vary depending on different decisions or different approaches. And it sounds like you will need to not only figure out what to do but also justify the approach you take when you report the results. I think all these things are true.</p>
<p>This is why Lotte Meteyard and I advise that researchers need to explain their approach, and share their data and code, when they report their analyses. Our best practice guidance for reporting mixed-effects models includes, among other things, the advice that in reports …</p>
<ul>
<li>Random effects are explicitly specified according to sampling units (e.g., participants, items), the data structure (e.g., repeated measures) and anticipated interactions between fixed effects and sampling units (e.g., intercepts only or intercepts and slopes). Fixed effects and covariates are specified from explicitly stated research questions and/or hypotheses.</li>
<li>Report the size of the sample analysed in terms of total number of data points and of sampling units (e.g., number of participants, number of items, number of other groups specified as random effects, such as classes of children).</li>
<li>A clear statement of the methods by which models are compared/selected; e.g., simple to complex, covariates first, random effects first, fixed effects first etc.</li>
<li>Report comparison method (LRT, AIC, BIC) and justify the choice.</li>
<li>A complete report of all models compared (e.g., in appendices/supplementary data/analysis scripts) with model equations and the result of comparisons.</li>
<li>If models fail to converge, the approach taken to manage this should be comprehensively reported. This should include the formula for each model that did or did not converge and a rationale for a) the simplification method used and b) the final model reported. This may be most easily presented in an analysis script.</li>
</ul>
<p>This looks like a lot of work.</p>
<ul>
<li>Why bother?</li>
</ul>
<p>I think it is always worth asking this question.</p>
<p>The first answer is that it is all relative. In my own experience, a lot of the effort spent in the research workflow used to be occupied by face-to-face data collection: weeks or months of testing; now all data get collected online, and it gets finished overnight. Considerable time and effort (as Hadley Wickham’s joke runs, 80% of analysis effort) was spent on tidying the data before analysis; I still do this work but it is now much faster and less effortful, thanks to `tidyverse:. A lot of effort used to be spent by me or colleagues on the literature review, the power analysis, or the stimulus preparation: that still happens. And a lot of effort used to be spent on doing the analysis and figuring out what the results mean: that, too, still happens. It is up to you if you want to spend ten months on data collection and five minutes on data analysis (as another joke has it, a million bucks on the data and a nickel on the statistics).</p>
<p>I think we <em>do</em> need to work at understanding the most appropriate analysis for our data, based on both our theoretical expectations and a data-driven evaluation. No-one is going to help us unsee multilevel structure in the data, or save us from the obligation to take into account random effects. Matuschek et al.&nbsp;(2017; p.312) argue that “The goal of model selection is not to obtain a significant p-value; the goal is to identify the most parsimonious model that can be assumed to have generated the data.” This makes sense to me. And their analyses show that determining a parsimonious model with a standard model selection criterion is a defensible choice, a way to take into account random effects, while controlling for both the risk of false positives, and the risk of false negatives.</p>
</section>
</section>
</section>
<section id="sec-glmm-reporting-results" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="sec-glmm-reporting-results"><span class="header-section-number">6.10</span> Reporting model results</h2>
<p>We have discussed how to report the results of mixed-effects models previously. The same conceptual structure, and similar language, can be used to report the results of Generalized Linear Mixed-effects Models (GLMMs).</p>
<p>I think you need to think about reporting the analysis as a task in which you first prepare the reader, explaining the motivation for using GLMMs, then present the analysis you did (the process, in outline), then present the results you shall later discuss.</p>
<ul>
<li>Start by explaining the study design – outline the fixed effects that have to be estimated</li>
<li>Explain how random effects structure was selected – be prepared to present a short version of the story in the main part of the report – sharing your code, in an appendix, to illustrate the steps in full</li>
</ul>
<p>You can see that I followed this progression of steps in the report I wrote for the published version of the analysis discussed, as an example, for this class (Monaghan et al., 2015; see following).</p>
<p>Lotte Meteyard and I recommend that results reporting should:</p>
<ul>
<li>Provide equation(s) that transparently define the reported model(s). An elegant way to do this is providing the model equation with the table that reports the model output.</li>
<li>And that final model(s) should be reported in a table that includes all parameter estimates for fixed effects (coefficients, standard errors and/or confidence intervals, associated test statistics and p-values if used), random effects (standard deviation and/or variance for each random effect, correlations/covariances if modelled) and some measure of model fit (e.g.&nbsp;R-squared, correlation between fitted values and data).</li>
<li>While researchers should be able to share the coding script used to complete the analysis and, wherever possible, share data that generated the reported results.</li>
</ul>
<p>For the word learning study we have been working through, the Results section for the report would include the following elements:</p>
<ul>
<li><p><strong>Explain approach</strong> We used mixed-effects models to analyse data because this approach permits modelling of both participant- and item-level variability simultaneously, unlike more traditional approaches such as ANOVA. In this study, multiple participants responded to multiple items, meaning that both participants and items were sources of nonindependence in our data (i.e.&nbsp;responses from the same participant are likely to be correlated, as are responses to the same item). Compared to ANOVA, mixed-effects models offer a more flexible approach, and are better able to handle missing data without significant loss of statistical power (Baayen, Davidson, &amp; Bates, 2008).</p></li>
<li><p><strong>Explain how you get from the study design to the model you use to test or estimate key effects</strong> We took a hypothesis driven approach, estimating the fixed effects of time (Time 1 versus Time 2), Orthography (absent versus present), Instructions (incidental versus explicit) and consistency (standardized H), as well as the interaction between orthography and instructions and the interaction between orthography and consistency. Different levels of the three binary fixed effects were sum coded. Consistency H, as a numeric predictor variable, was standardized to z scores before entry to models as a predictor.</p></li>
</ul>
<!-- We took a hypothesis-driven approach, estimating the fixed effects of time (Time 1 versus Time 2), orthography (absent versus present), instructions (incidental versus explicit) and consistency (standardized H), as well as the interaction between orthography and instructions and the interaction between orthography and consistency. Different levels of the three binary fixed effects were sum coded, with orthography as $−1$ (absent) versus $+1$ (present), instructions as $−1$ (incidental) versus $+1$ (explicit), and time as $−1$ (Time 1) versus $+1$ (Time 2). Consistency H, as a numeric predictor variable, was standardized to z scores before entry to models as predictors. -->
<ul>
<li><p><strong>Outline the model comparison or model selection work</strong> The models were initially fitted specifying just random effects to account for variation by participants and stimuli in accuracy (random intercepts) plus terms to estimate the fixed effects of the experimental conditions ([name them]), and the interactions [name them]. Following the recommendations of Barr, Levy, Scheepers, and Tily (2013; see also Baayen, 2008; Matuschek et al., 2017), we fitted further models adding both random intercepts and random slopes for the random effects. Likelihood ratio test comparison of models showed that a model with both random intercepts and slopes … fit the data better than a model with just random intercepts <span class="math inline">\((\chi^2(df) = ..., p = ...)\)</span>.</p></li>
<li><p>Use appendices or supplementary materials] To give the reader full information on models fit, model comparisons</p></li>
<li><p>Help the reader with a concise summary of estimates] As I have advised for reporting linear models, I included a tabled summary of coefficient estimates, presenting fixed and random effects (see e.g.&nbsp;Davies et al., 2013; Monaghan et al., 2015)</p></li>
<li><p>Show and tell] Use figures – model prediction plots, as seen – to help the reader to see what the fixed effects estimates imply.</p></li>
<li><p><strong>Which model do we report?</strong></p></li>
</ul>
<p>Note that given the model comparison results we have seen, I would probably report the estimates from `long.orth.3.glmer:. The model appears to include the most comprehensive account of random effects while still being capable of converging.</p>

<!-- As I have advised for reporting linear models, I included a tabled summary of coefficient estimates, as shown in Figure \ref{noun-verb-learning-glmm-paper-summary} -->
<!-- \begin{figure}[h!] -->
<!-- \centering -->
<!--      \includegraphics[scale=0.45]{noun-verb-learning-glmm-paper-summary} -->
<!--      \caption{Summary of model estimates for the word learning study data} -->
<!--      \label{noun-verb-learning-glmm-paper-summary} -->
<!--     \end{figure} -->
<!-- You can see that: -->
<!--  -->
<!-- \item -->
<!-- I report fixed and random effects -->
<!-- \item -->
<!-- I report the model formula -->
<!-- \item -->
<!-- I report confidence intervals -->
<!--  -->
<!-- I wrote the paper a while ago.  -->
<!-- I think, now, I would add an effect plots to help in model explanation, see Figure \ref{noun-verb-learning-glmm3-partial-effects-sjplot-plot-2}. -->
<!-- The plot would be included to show the predicted response accuracy given the effect of learning block, and the fact that the effect of block is different for different experimental conditions. -->
<!-- I think the visualisation works because it shows that participants can learn the artificial language they were asked to learn but that, clearly, verbs are harder to learn. -->
<!-- \begin{figure}[h!] -->
<!-- \centering -->
<!--       \includegraphics[scale=0.65]{noun-verb-learning-glmm3-partial-effects-sjplot-plot} -->
<!--       \caption{Gavagai word learning study: plot showing the interaction between the block and experimental condition effects} -->
<!--       \label{noun-verb-learning-glmm3-partial-effects-sjplot-plot-2} -->
<!--   \end{figure} -->
<!-- Random effects plots may help the reader to understand model selection choices. -->
<!-- We do not usually see random effects plots in journal articles but it may be worth presenting such plots where a fixed effect is not significant but the variance associated with random differences in the slope of the fixed effect is warranted (say, as indicated by the results of an LRT comparison). -->
<!-- In the case of this example, inspection of the plot showing the random differences between subjects in intercepts and in the slope of the block effect shows that while there are big differences between participants in intercepts, there are not many differences in the slope of the block effect (Figure \ref{noun-verb-learning-glmm3-ggcaterpillar-subjects}). -->
<!-- \begin{figure}[h!] -->
<!-- \centering -->
<!--      \includegraphics[scale=0.65]{noun-verb-learning-glmm3-ggcaterpillar-subjects} -->
<!--      \caption{Plotting showing the predicted deviations (random effects) associated with rnadom differences between the overall average intercept and the intercept per participant, as well as differences between the average fixed effect of block and the effect of block per participant} -->
<!--      \label{noun-verb-learning-glmm3-ggcaterpillar-subjects} -->
<!--     \end{figure} -->
<!-- #### Exercise: specifying a model to estimate fixed effects and to account for random effects adequately and parsimoniously {#gavagai} -->
<!-- -- see code in: -->
<!-- 402-class-9-110319-full-model-fitting-notes.R -->
<!-- /Users/robdavies/OneDrive - Lancaster University/FromBox/teaching-PSYC401-402/PSYC402 practical materials -->
<!-- -- see also: -->
<!-- PG-methods-class-nine.Rmd -->
<!-- The best way to learn the methods discussed here is to try them out with a different study, different research questions, and a different dataset. -->
<!-- To do this, we can look at the data from the Monaghan et al. (2015) study, discussed at the start. -->
<!-- Monaghan et al. (2015) compared learning of noun-object pairings, verb-motion pairings, and learning of both noun and verb pairings simultaneously, using an identical cross-situational learning task and environment in each case.  -->
<!-- We predicted that, consistent with previous studies of noun learning, word-object mappings would be learned from cross-situational statistics even when the utterances were complex and the observed dynamic scenes more complex than stationary figures.  -->
<!-- We also predicted that it would be possible to learn verbs from the same cross-situational statistics even though the names for the objects remained unknown, and that when nouns and verbs could be learned from the same utterance then cross-situational statistics would be sufficiently powerful to demonstrate learning for both these grammatical categories. -->
<!-- ##### Study information -->
<!--  -->
<!--     \item -->
<!--     48 adults participated in a set of learning trials: each person did 12 blocks of 24 trials -->
<!--     \item -->
<!--     In each trial, participants observed two objects undergoing a different motion (one on the left, one on the right), and heard a sentence of fake words     -->
<!--     \item -->
<!--     Words were either assigned to "refer to" the objects (nouns) or to the motions (verbs) -->
<!--     \item -->
<!--     The task was to indicate whether the heard sentence referred to the action on the left or on the right of the screen. -->
<!--     \item -->
<!--     This task was designed to test if participants could learn the object-to-word or the motion-to-word associations -->
<!--    -->
<!-- The experimental stimuli are illustrated in Figure \@ref(fig:noun-verb-learning-stimulus). -->
<!-- ```{r noun-verb-learning-stimulus, echo=FALSE, fig.cap="Example of a learning trial: two moving objects are observed; arrows indicate the movement path of the object; the four word phrase is simultaneously heard, with 'tha' and 'noo' function words and 'makkot' and or 'pakrid' referring to the motion and or object, according to condition, in one of the scenes", out.width = '65%'} -->
<!-- knitr::include_graphics("noun-verb-learning-study-methods-stimulus.pdf") -->
<!-- ``` -->
<!-- The function words provided distributional information in the task such that they indicated the role of the word they preceded.  -->
<!-- Thus, in the noun-only condition, one of the function words always preceded the object referring word and the other function word preceded the non-referring words.  -->
<!-- In the verb-only condition, one of the function words preceded the motion-referring word and the other preceded the non-referring word.  -->
<!-- In the noun-and-verb condition, one of the function words preceded the object referring word and the other function word preceded the motion referring word.  -->
<!-- We selected function words to precede both nouns and verbs in the speech in order to control the distributional information for nouns and verbs.  -->
<!-- If we look at the proportion of responses that were produced correctly in each condition, see Figure \@ref(fig:word-learning-lm-per-subject-for-report), we can see how the proportion of correct responses increases as participants did more and more learning trials. -->
<!-- We can also see that the rate at which learning improves is different depending on whether participants were learning noun-object, verb-motion, or noun-object and verb-motion combinations. -->
<!-- ```{r gavagai-accuracy-histogram, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Histogram showing frequency of correct vs. incorrect responses per subject per learning trials block and experimental learning condition, in the Monaghan et al. (2014) study", out.width='90%', fig.width=10} -->
<!-- paccuracy <- ggplot(data = gavagai, aes(x = accuracy, fill = accuracy)) -->
<!-- paccuracy +  -->
<!--   geom_histogram(stat = "count") +  -->
<!--   theme_bw() + -->
<!--   theme(axis.title.x = element_text(size=25)) +  -->
<!--   theme(axis.text.x = element_text(size=20)) + -->
<!--   theme(axis.title.y = element_text(size=25)) +  -->
<!--   theme(axis.text.y = element_text(size=20)) + -->
<!--   ggtitle("Response accuracy") +  -->
<!--   theme(title = element_text(size=30)) + -->
<!--   facet_grid(Experiment ~ block) -->
<!-- ``` -->
<!-- Given that the outcome, here, is the accuracy of response, and that the outcome variable has values, for each person's response in each trial, that can only be 0 (incorrect) or 1 (correct), we must use a logistic model. -->
<!-- The fact that outcomes consist of the observation whether a response is correct or incorrect means that the outcome variable is a categorical (here a binary) variable. -->
<!-- In addition, we must take into account the fact that our data have a multilevel structure. -->
<!-- This is true for the word learning data set, because the data were collected according to a repeated measures design, with every participant experiencing every condition (block and learning condition are within-subjects), and every participant responding to every stimulus. -->
<!-- This structure -- where we have multiple observed responses for every stimulus, and multiple observed responses by every participant -- requires the use of mixed-effects models. -->
<!-- Thus, we need to use Generalized Linear Mixed-effects Models (GLMMs). -->
<!-- ##### Access the data -->
<!-- Go to the 402 Moodle folder for Week 19, find and download the data file we need from: -->
<!-- \url{https://modules.lancaster.ac.uk/mod/folder/view.php?id=1785576} -->
<!-- We will be working with the noun verb learning study data: -->
<!-- * noun-verb-learning-study.csv -->
<!-- Read it into R: -->
<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- gavagai <- read_csv("noun-verb-learning-study.csv", na = "-999") -->
<!-- ``` -->
<!-- ##### Prepare the data -->
<!-- Sometimes, a `glmer(): warning message advises us to consider rescaling the continuous numeric predictor variable. -->
<!-- To avoid getting that warning, we should center the block variable before the modeling using the `scale(): function. -->
<!-- ```{r} -->
<!-- gavagai$cblock <- scale(gavagai$block, scale = FALSE, center = TRUE) -->
<!-- ``` -->
<!-- In this transformation}, we are asking R to create a new variable `all.merged$cblock: from the old variable, by calculating, for each block, the difference between the block identity number (there are 12 blocks, so blocks are numbered 1-12) and the average block identity number. -->
<!-- This transformation makes later interpretation of effects a bit easier (see, e.g., Cohen et al., 2003) on why centering predictor variables helps you to interpret the estimated coefficients of effects. -->
<!-- ##### Work on the model -->
<!-- To get you started, we can specify a random intercepts model. -->
<!-- We can start with a model for which we specify (1.) that `accuracy: is the outcome variable, and we are predicting it with (2.) the fixed effects plus (3.)just the random effects on intercepts of participant `Subjecta:, stimulus object `targetobject: and stimulus action `targetaction:. -->
<!-- Now the model code. -->
<!-- ```{r eval=FALSE} -->
<!-- gavagai.glmer <- glmer(accuracy ~ -->
<!--           Experiment*cblock + -->
<!--           (1|Subjecta) + (1|targetobject) + (1|targetaction), -->
<!--           data = gavagai, family = binomial, -->
<!--           glmerControl(optimize = "bobyqa")) -->
<!-- summary(gavagai.glmer) -->
<!-- ``` -->
<!-- In this code chunk, we have:  -->
<!--  -->
<!--   \item -->
<!--    `glmer(): the function name changes because we want a generalized} linear mixed-effects model -->
<!--   \item -->
<!--   With `(1|Subjecta) + (1|targetobject) + (1|targetaction): we have included random effects of stimulus object and stimulus action (motion) as well as participants on on intercepts -->
<!--    \item -->
<!--   `family = binomial: accuracy is a binary outcome variable so assume a binomial probability distribution -->
<!--   \item -->
<!--   `glmerControl(optimize = "bobyqa")): we change the underlying mathematical engine (the optimizer) to cope with greater model complexity -->
<!--     \item -->
<!--    `Experiment: learning conditions coded with the "Experiment" variable, a factor with levels `nounonly:, `verbonly:, and `nounverb: -->
<!--      \item -->
<!--    `block: there were 12 blocks of 24 learning trials, and here block is treated as a numeric variable -->
<!--    \item -->
<!--    `cblock: notice that we use the centred} block variable `cblock: -->
<!--   \item -->
<!--    Notice that the `(something)*(something): get you interactions and main effects for all possible pairs of variables -->
<!--  -->
<!-- Now it's over to you: what next? -->
</section>
<section id="sec-glmm-summary" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="sec-glmm-summary"><span class="header-section-number">6.11</span> Summary</h2>
<p>We focused on the need to use Generalized Linear Mixed-effects Models (GLMMs). We identified the kind of outcome data (like response accuracy) that requires analysis using GLMMs. Alternative methods, and their limitations, were discussed.</p>
<p>We examined a study that incorporates repeated measures (participants respond to multiple stimuli), a 2 x 2 factorial design, and a longitudinal aspect (participants tested at two time points), the word learning study (Ricketts et al., in press).</p>
<p>We discussed the need to use effect coding for categorical predictor variables (factors). We work through example code to set factor level coding as required.</p>
<p>We worked through a random intercepts GLMM, and identified the critical elements of the model code, and of the results summary, including hypothesis test p-values. We examined how to present visualizations of fixed effects estimates (model predictions) using different libraries.</p>
<p>We then moved on to considering the question of what random effects we should include in the model. We considered the study design in some depth, and explored what random effects we could, in theory, expect to require. We then worked through a model comparison approach. We looked at some warning signs, what they indicate, and how to deal with them.</p>
<p>We considered how to report the model selection (or comparison, or building) process, and how to report the model for presentation of results.</p>
<section id="sec-glmm-glossary-useful-functions" class="level3" data-number="6.11.1">
<h3 data-number="6.11.1" class="anchored" data-anchor-id="sec-glmm-glossary-useful-functions"><span class="header-section-number">6.11.1</span> Glossary: useful functions</h3>
<p>We used two functions to fit and evaluate mixed-effects models.</p>
<ul>
<li>We used <code>glmer()</code> to fit a mixed-effects model</li>
<li>We used <code>anova()</code> to compare two or more models using AIC, BIC and the Likelihood Ratio Test</li>
</ul>
</section>
</section>
<section id="r-code-and-data-file-access-for-the-class" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="r-code-and-data-file-access-for-the-class"><span class="header-section-number">6.12</span> R code and data file access for the class</h2>
<p>Activities in the class that goes with this chapter are associated with the following data file and .R code file:</p>
<p><code>04-glmm-workbook.R: \item</code>long.orth_2020-08-11.csv:</p>
<!--  -->
<!-- \item -->
<!-- `04-glmm-workbook.R: -->
<!-- \item -->
<!-- `04-glmm-exercises.R: -->
<!-- \item -->
<!-- `long.orth_2020-08-11.csv: -->
<!-- \item -->
<!-- noun-verb-learning-study.csv -->
<!--  -->
<p>You can download the data and the .R code files from here:</p>
<p></p>
<p>Run the code in the .R file to reproduce the results presented in this chapter and in the slides.</p>
<p>You can also see resources that you can use, optionally, to extend your practice and deepen your understanding, by exploring the analysis of the noun-verbl learning “Gavagai” study data (Monaghan et al., 2015). These resources can be downloaded as part of the same folder and comprise a dataset with an extensively commented .R analysis script.</p>
<ul>
<li><code>402-04-GLMM-exercise-gavagai-data-analysis-notes.R</code></li>
<li><code>noun-verb-learning-study.csv</code></li>
</ul>
</section>
<section id="sec-glmm-recommended-reading" class="level2" data-number="6.13">
<h2 data-number="6.13" class="anchored" data-anchor-id="sec-glmm-recommended-reading"><span class="header-section-number">6.13</span> Recommended reading</h2>
<p>The example studies referred to in this chapter are published in (Monaghan et al., 2015; Ricketts et al., in press).</p>
<p>Ben Bolker provides a very readable introduction to Generalized Linear Mixed-effects Models (Bolker et al., 2009; see also Jaeger, 2008).</p>
<p>Baayen et al.&nbsp;(2008; see, also, Barr et al., 2013) discuss mixed-effects models with crossed random effects.</p>
<p>The issue of model comparison or model selection, and the appropriate choice of random effects structure is discussed by Baayen (Baayen et al., 2008; Barr et al., 2013; Bates et al., 2015; Eager &amp; Roy, 2017; Matuschek et al., 2017).</p>
<p>I wrote a tutorial article on mixed-effects models with Lotte Meteyard (Meteyard &amp; Davies, 2020). We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.</p>
<p>Book length introductions are provided by Snijders and Bosker (2012; good introduction in simple language), Gelman and Hill (2006; very influential), and Pinheiro and Bates (2000; critical for lme4).</p>
<section id="a-very-useful-faq" class="level3" data-number="6.13.1">
<h3 data-number="6.13.1" class="anchored" data-anchor-id="a-very-useful-faq"><span class="header-section-number">6.13.1</span> A <em>very</em> useful FAQ</h3>
<p>Can be found here:</p>
<p></p>
<!-- -- see also: -->
<!-- https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html -->
<!-- https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html -->
</section>
<section id="references-list" class="level3" data-number="6.13.2">
<h3 data-number="6.13.2" class="anchored" data-anchor-id="references-list"><span class="header-section-number">6.13.2</span> References list</h3>
<p>Baayen, R. H. (2008). Analyzing linguistic data. A practical introduction to statistics using R}. CUP.</p>
<p>Baayen, R. H., Davidson, D. J., &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. <em>Journal of Memory and Language</em>, 59, 390-412.</p>
<p>Baguley, T. (2012). <em>Serious stats: A guide to advanced statistics for the behavioral sciences.</em> Macmillan International Higher Education.</p>
<p>Balota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B., . . . Treiman, R. (2007). The English lexicon project. <em>Behavior Research Methods</em>, 39, 445– 459. http://dx.doi.org/10.3758/BF03193014</p>
<p>Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. <em>Journal of Memory and Language</em>, 68, 255-278.</p>
<p>Bates, D., Kliegl, R., Vasishth, S., &amp; Baayen, H. (2015). Parsimonious mixed models. <em>arXiv preprint</em> arXiv:1506.04967.</p>
<p>Bolker, B. M., Brooks, M. E., Clark, C. J., Geange, S. W., Poulsen, J. R., Stevens, M. H. H., &amp; White, J. S. S. (2009). Generalized linear mixed models: A practical guide for ecology and evolution. <em>Trends in Ecology &amp; Evolution</em>, 24, 127–135.</p>
<p>Chambré, S. J., Ehri, L. C., &amp; Ness, M. (2017). Orthographic facilitation of first graders’ vocabulary learning: does directing attention to print enhance the effect? <em>Reading and Writing</em>, 1-20. doi:10.1007/s11145-016-9715-z</p>
<p>Cohen, J., Cohen, P., West, S.G., &amp; Aiken, L.S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences}. Hillsdale, NJ: Lawrence Erlbaum Associates.</p>
<p>Colenbrander, D., Miles, K. P., &amp; Ricketts, J. (2019). To See or Not to See: How Does Seeing Spellings Support Vocabulary Learning? <em>Language, Speech, and Hearing Services in Schools</em>, 50(4), 609-628. doi:10.1044/2019_LSHSS-VOIA-18-0135</p>
<p>Eager, C., &amp; Roy, J. (2017). Mixed effects models are sometimes terrible. arXiv preprint</p>
<p>Ehri, L. C. (2014). Orthographic mapping in the acquisition of sight word reading, spelling memory, and vocabulary learning. <em>Scientific Studies of Reading</em>, 18(1), 5-21. doi:10.1080/10888438.2013.819356</p>
<p>Gelman, A., &amp; Hill, J. (2006). <em>Data analysis using regression and multilevel/hierarchical models</em>. New York, NY: Cambridge University Press. http://dx.doi.org/10.1017/CBO9780511790942</p>
<p>Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language}, 59(4), 434-446.</p>
<p>Krepel, A., de Bree, E. H., &amp; de Jong, P. F. (2020). Does the availability of orthography support L2 word learning? <em>Reading and Writing</em>. doi:10.1007/s11145-020-10078-6</p>
<p>Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. *Behavior research method, 49, 1494–1502.</p>
<p>Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp; Bates, D. (2017). Balancing Type I error and power in linear mixed models. <em>Journal of Memory and Language</em>, 94, 305-315.</p>
<p>Mengoni, S. E., Nash, H., &amp; Hulme, C. (2013). The benefit of orthographic support for oral vocabulary learning in children with Down syndrome. <em>Journal of Child Language</em>, 40 (Special Issue 01), 221-243. doi:10.1017/S0305000912000396</p>
<p>Monaghan, P., Mattock, K., Davies, R. A., &amp; Smith, A. C. (2015). Gavagai is as gavagai does: Learning nouns and verbs from Cross‐Situational statistics. Cognitive Science}, 39, 1099-1112.</p>
<p>Mousikou, P., Sadat, J., Lucas, R., &amp; Rastle, K. (2017). Moving beyond the monosyllable in models of skilled reading: Mega-study of disyllabic nonword reading. <em>Journal of Memory and Language</em>, 93, 169-192. doi:http://dx.doi.org/10.1016/j.jml.2016.09.003</p>
<p>Pinheiro, J. C., &amp; Bates, D. M. (2000). <em>Mixed-effects models in S and S-PLUS</em>. New York, NY: Springer-Verlag.</p>
<p>Raaijmakers, J. G., Schrijnemakers, J. M., &amp; Gremmen, F. (1999). How to deal with “the language-as-fixed-effect fallacy”: Common misconceptions and alternative solutions. <em>Journal of Memory and Language</em>, 41, 416-426.</p>
<p>Ricketts, J., Dawson, N., &amp; Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. <em>Learning and Instruction</em>, 74, 101468.</p>
<p>Snijders, T.A., &amp; Bosker, R.J. (2012). <em>Multilevel analysis (2nd Edition)</em>. London, UK: Sage.</p>
<!-- -- superfluous materials -- -->
<!-- -- see also: -->
<!-- PG-methods-class-eight.Rnw -->
<!-- 2021-02-14-intro-mixed-effects.Rmd -- commented out parts at the end -->
</section>
</section>
<section id="appendix-example-dataset-variable-information" class="level2" data-number="6.14">
<h2 data-number="6.14" class="anchored" data-anchor-id="appendix-example-dataset-variable-information"><span class="header-section-number">6.14</span> Appendix: Example dataset variable information</h2>
<p>Further information about the variables in the `long.orth_2020-08-11.csv: dataset.</p>
Cell values comprise character strings coding for participant. Participant identity codes were used to anonymise participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded “EOF[number]”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “ND[number]”.
Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.
Observations taken for children included in studies 1 and 2 – participants in the longitudinal daa collection – were coded “Study1&amp;2”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “Study2”.
Variable coding for whether participants undertook training in the explicit} or incidental} conditions.
Experiment administration coding
Letter string values show the words presented as stimuli to children.
Calculated orthography-to-phonology value for each word.
Variable coding for whether participants had seen a word in training in the orthography absent} or present} conditions.
Variable coding for the post-test measure: <code>Sem_all: if the semantic post-test;</code>Orth_sp: if the orthographic post-test.
<p>Variable coding for response category. For the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:</p>
3 – correct response in the definition task
2 – correct response in the cued definition task
1 – correct response in the recognition task
<p>0 – if the item wasn’t correctly defined or recognised</p>
<p>For the orthographic post-test, responses were scored as:</p>
1 – correct, if the target spelling was produced in full
<p>0 – incorrect</p>
Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence
Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds.
Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds.
Raw score – Castles and Coltheart Test 2; number of regular words read correctly
Raw score – Castles and Coltheart Test 2; number of irregular words read correctly
Raw score – Castles and Coltheart Test 2; number of nonwords read correctly
Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II
Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition
Transcription of the spelling response produced by children in the orthographic post-test
Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure, using the `stringdist: library (van der Loo, 2019). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.
We standardized TOWREsweRS values, calculating the z score as <span class="math inline">\(z = \frac{x - \bar{x}}{sd_x}\)</span>, over all observations in the longitudinal} (Study 1) or concurrent} (Study 2) data-set, using the <code>scale()</code> function in R.
Standardized TOWREpdeRS scores
Standardized CC2regRS scores
Standardized CC2irregRS scores
Standardized CC2nwRS scores
Standardized WASIvRS scores
<p>Standardized BPVSRS scores</p>
<!-- \item[mean-z-vocab] -->
<!-- We aggregate vocabulary measures, averaging variables together: `data$mean_z_vocab <- (data$zWASIvRS + data$zBPVSRS)/2: -->
<!-- \item[mean_z_read] -->
<!-- We aggregated reading skills measures, averaging variables together: \verb-data$mean_z_read<-(data$zTOWREsweRS + data$zTOWREpdeRS + data$zCC2regRS + data$zCC2irregRS + data$zCC2nwRS)/5- -->
<!-- \item[zConsistency-H] -->
<!-- Standardised `Consistency_H: scores -->


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Note that, the strengths of mixed-effects models mean we do not need complete data for responses to all stimuli, under all conditions, for each participant; the method can tolerate imbalances in the data. Likewise, if we are interested in, say, the slope of a numeric variable like, as in the example data, word spelling-sound consistency, we do not need to worry about having complete data, for each person, for responses at each level of the consistency variable. We just need enough data, observed responses at different levels of the variable, to be able to estimate the slope of the variable.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-mixed.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Developing linear mixed-effects models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./intro.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction: the why</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>