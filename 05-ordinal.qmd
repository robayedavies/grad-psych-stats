---
bibliography: references.bib
---

# Introduction to Ordinal Models {#sec-ordinal-intro}

```{r libraries-hide}
#| warning: false
#| echo: false
library(ggdist)
library(ggeffects)
library(here)
library(lme4)
library(memisc)
library(ordinal)
library(patchwork)
library(tidyverse)
```

```{r}
#| label: readin
#| message: false
#| echo: false
# -- Health comprehension (2021-22 student project data):
health <- read_csv("2021-22_PSYC304-health-comprehension.csv", 
                                 na = "-999",
                                 col_types = cols(
                                   ResponseId = col_factor(),
                                   rating = col_factor(),
                                   GENDER = col_factor(),
                                   EDUCATION = col_factor(),
                                   ETHNICITY = col_factor(),
                                   NATIVE.LANGUAGE = col_factor(),
                                   OTHER.LANGUAGE = col_factor(),
                                   text.id = col_factor(),
                                   text.question.id = col_factor(),
                                   study = col_factor()
                                 )
                               )

health.subjects <- health %>% 
  group_by(ResponseId) %>%
  mutate(mean.self = mean(as.numeric(rating))) %>%
  ungroup() %>%
  distinct(ResponseId, .keep_all = TRUE) %>%
  select(-c(rating:text.question.id))
```

::: callout-warning
This chapter is under construction
:::

## Motivations: working with ordinal outcomes {#sec-ordinal-motivations}

Ordinal data are very common in psychological science. Often, we will encounter ordinal data recorded as responses to Likert-style items in which the participant is asked to indicate a response on an ordered scale ranging between two end points [@bürkner2019; @liddell2018]. An example of a Likert question item might be: *How well do you think you have understood this text? (Please check one response)* where the participant must respond by checking an option, given 5 options ranging from 1 (not well at all) to 5 (very well). The critical characteristics of such responses are that:

-   The responses are ordered, as indicated by the number labels;
-   Response types are categorical or qualitative, not numeric.

We will be working with study data in which the outcome that is the target for our analyses comprise responses to questions designed to elicit ratings. Ordinal data may, however, also derive from situations in which ordered categorical responses do not derive from ratings items [we will look briefly at sequential responses, @bürkner2019].

The challenge we face is that we will aim to develop skills in using *ordinal models* when, in contrast, most psychological research articles will report analyses of ordinal data using conventional methods like ANOVA or linear regression. We will work to understand why ordinal models are better. We will learn that applying conventional methods to ordinal data will, in principle, involve a poor account of the data and, in practice, will create the risk of producing misleading results. And we will learn how to work with and interpret the results from ordinal models with or without random effects.

In our work in this chapter, we will rely extensively on the ideas set out by @liddell2018, see @sec-ordinal-recommended-reading.

## The key idea to get us started {#sec-ordinal-ideas}

::: callout-important
Ordinal responses are labelled with numbers but ordinal data are *not* numeric.
:::

Ordinal responses are coded with numeric labels. These number labels may indicate order but we do not know that the difference between e.g. response options `1` versus `2` is the same as the difference between `2` versus `3` or `3` versus `4`. Ordinal data contrast with *metric* data [@liddell2018] which are recorded on scales for which we assume *both* order and equal intervals. When researchers apply metric models to ordinal data, they incorrectly assume that the response options e.g. in ratings are separated by equal intervals. Yet, in a review of the 68 recently articles that mentioned the term "Likert" in a sample of highly ranked Psychology journals, @liddell2018 found that ordinal data were treated as metric and the articles presented results from metric models.

One way to think about ordinal data is that often (but not always) ratings may be understood to come from psychological processes in which the participant, in response to the Likert question, divides some latent (unobserved) psychological continuum or scale into categories in order to select a response option. Imagine, for example, that you have been asked the question "How well do you understand this text? (on a scale from 1-5)". Presumably, to answer this question, you will have to choose a response based on where you think you are on your unobserved measure of your understanding.

You may be able to evaluate the cohesion, or some other internal measure, of your understanding of the text. Simplifying a bit, we might assume that your internal measure of understanding is associated with a normal probability distribution so that it peaks over some value (e.g., `3`) of the strength of understanding though other values are possible. As @fig-latent-normal-splits suggests, a participant in this common situation will have to map the internal measure (the latent scale, e.g., of understanding) to a number from the response options you are given (e.g., rating scale values ranging 1-5). But there is no reason to suppose that your internal measure of your understanding is divided into an ordered metric scale.

```{r}
#| label: fig-latent-normal-splits
#| fig-cap: "A latent scale on the horizontal axis is divided into intervals or bins divided by thresholds marked by dotted lines. The cumulative normal probability in the intervals is the probability of the ordinal values."
#| fig-alt: "The figure shows a normal curve, split by dotted lines shown at x-axis points labelled 1-5."
#| warning: false
#| message: false
#| echo: false
#| fig-height: 5
#| fig-width: 5
ggplot(data = data.frame(x = c(-5, 5)), aes(x)) +
  stat_function(fun = dnorm, n = 100, 
                args = list(mean = 0, sd = 1)) + 
  ylab("") +
  xlab("Rating scale") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = c(-2, -1.5, 0, 1.5, 2), 
                     labels = c("1", "2", "3", "4", "5")) +
  geom_vline(xintercept = c(-2, -1.5, 0, 1.5, 2),
             linetype = "dotted", 
             colour = "red") +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
```

In conducting analyses of ordinal data with ordinal models, we often fit models that describe the cumulative probability that a rating response is located at some value (typically, understood in terms of threshold) on an underlying latent continuum. In ordinal models, we do not assume that the ordinal responses map to equally spaced intervals on the latent scale: the values or thresholds at which the continuum are split are to be estimated.

In applying metric models to ordinal data, we *do* assume that intervals are equal though this assumption is unlikely to be true or, at least, is unlikely to be verifiable. This faulty assumption has consequences. Because the mis-application of metric models (e.g. ANOVA, linear models) to ordinal data is both commonplace and *risky*. As @liddell2018 demonstrate, mis-applying metric models to ordinal data can result in false positives (detecting a difference when none is present), false negatives (missing a difference that is present) and inversions (swapping the difference so that is positive instead of negative or vice versa).\
These kinds of misrepresentions cannot be avoided and are not fixed by, for example, averaging ratings scales data together.

## Targets {#sec-ordinal-targets}

1.  Understand practically the reasons for using ordinal models when we analyze ordinal outcome variables, @sec-ordinal-practical-understanding.
2.  Practice running ordinal models with varying random effects structures.
3.  Practice reporting the results of ordinal models, including through the use of prediction plots.

## Study guide {#sec-ordinal-study-guide}

I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.

**1. Chapter: 05-ordinal**

1.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with ordered categorical outcomes i.e. *ordinal data* using ordinal models.

1.2. The practical elements include data tidying, visualization and analysis steps.

1.3. You can read the chapter, run the code, and do the exercises.

-   Read in the example dataset.
-   Experiment with the .R code used to work with the example data.
-   Run ordinal models of demonstration data.
-   Run ordinal models of alternate data sets.
-   Review the recommended readings (@sec-ordinal-recommended-reading).

**2. Practical workbook materials**

2.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning.

## The data we will work with: {#sec-ordinal-data}

We will be working, at first, with a sample of data collected as part of the **Clearly understood: health comprehension** project (Davies, Ratajczak, Gillings, Chadwick & Gold). These data are unpublished.

### Study information {#sec-ordinal-data-study}

#### Introduction: the background for the study {#sec-ordinal-data-background}

Our interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information. In part, we are concerned about the processes that health providers or clinicians apply to assure the effectiveness of the text they produce to guide patients or carers, for example, in taking medication, in making treatment decisions, or in order to follow therapeutic programmes.

It is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like "How well do you understand this text?" This kind of question presents a metacognitive task: we are asking a participant *to think about their thinking*. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions [@dunlosky2007].

For these reasons, we conducted studies in which we presented adult participants with sampled health information texts (taken from health service webpages) and, critically, asked them to respond to the question:

-   `How well do you think you have understood this text? (Please check one response)`

For each text, in response to this question, participants were asked to click on one option from an array of response options ranging from (1) `Not well at all` to (9) `Extremely well`. The data we collected in this element of our studies comprise, clearly, *ordinal responses*. Thus, we may use these data to address the following research question.

::: callout-note
- What factors predict self-evaluated *rated* understanding of health information.
:::

#### Participants {#sec-ordinal-data-participants}

We will work with a sample of participant data drawn from a series of Lancaster University undergraduate dissertation studies connected to the **Clearly understood** project.
In these studies, we collected data from `r length(health.subjects$ResponseId)` participants on a series of measures (@sec-ordinal-data-materials-procedure) of vocabulary knowledge, health literacy, reading strategy, as well as responses to health information texts.
The distributions of participants' scores on each of a range of attribute variables

```{r}
#| warning: false
#| echo: false
#| label: fig-histogram-grid-
#| fig-cap: "Grid of plots showing the distribution of participant attributes. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); and age (years). We also see dot plots presenting counts of numbers of participants of different self-reported gender, education, and ethnicity categories."
#| fig-alt: "The figure presents a grid of histograms indicating the distribution of (x-axis) scores on a range of participant attribute variables. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); age (years); gender; education, and ethnicity. The plots indicate: (1.) most self-rated accuracy scores are high (over 6); (2.) many participants with vocabulary scores greater than 30, a few present lower scores; (3.) health literacy scores centered on 8 or some, with lower and higher scores; (4.) a skewed distribution of reading strategy scores, with many around 20-40, and a tail of higher scores; (5.) most participants are 20-40 years of age, some older; (6.) many more female than male participants, very few non-binary reported; (7.) many more participants with higher education than further, very few with secondary; and (8.) many White participants (ONS categories), far fewer Asian or Mixed or Black ethnicity participants."
#| fig-width: 12
#| fig-height: 7

p.self <- ggplot(data = health.subjects, aes(x = mean.self)) + 
  geom_histogram(binwidth = 1) +
  theme_bw() +
  labs(x = "Self-rated accuracy", y = "frequency count")   

p.shipley <- ggplot(data = health.subjects, aes(x = SHIPLEY)) + 
  geom_histogram(binwidth = 2) +
  theme_bw() +
  labs(x = "Vocabulary (SHIPLEY)", y = "frequency count")   

p.HLVA <- ggplot(data = health.subjects, aes(x = HLVA)) + 
  geom_histogram(binwidth = 2) +
  theme_bw() +
  labs(x = "HLVA", y = "frequency count") 

p.FACTOR3 <- ggplot(data = health.subjects, aes(x = AGE)) + 
  geom_histogram(binwidth = 5) +
  theme_bw() +
  labs(x = "Reading strategy (FACTOR3)", y = "frequency count") 

p.age <- ggplot(data = health.subjects, aes(x = AGE)) + 
  geom_histogram(binwidth = 5) +
  theme_bw() +
  labs(x = "Age (years)", y = "frequency count") 

p.GENDER <- health.subjects %>%
  mutate(GENDER = fct_recode(GENDER, 
                             
            "Prefer-not-to-say" = "prefer-not-to-say"                 
                             
                             )) %>%
  group_by(GENDER) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(GENDER = fct_reorder(GENDER, count)) %>%
  ggplot(aes(y = GENDER, x = count)) +
  geom_point(size = 3, aes(colour = GENDER)) +
  theme_bw() + 
  labs(y = "Gender", x = "frequency count") 

p.EDUCATION <- health.subjects %>%
  group_by(EDUCATION) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(EDUCATION = fct_reorder(EDUCATION, count)) %>%
  ggplot(aes(y = EDUCATION, x = count)) +
  geom_point(size = 3, aes(colour = EDUCATION)) +
  theme_bw() + 
  labs(y = "Education", x = "frequency count") 

p.ETHNICITY <- health.subjects %>%
  group_by(ETHNICITY) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(ETHNICITY = fct_reorder(ETHNICITY, count)) %>%
  ggplot(aes(y = ETHNICITY, x = count)) +
  geom_point(size = 3, aes(colour = ETHNICITY)) +
  theme_bw() + 
  labs(y = "Education", x = "frequency count")

p.self + p.shipley + p.HLVA + p.FACTOR3 + p.age + p.GENDER + p.EDUCATION + p.ETHNICITY + 
  plot_layout(ncol = 3)
```

#### Stimulus materials and data collection procedure {#sec-ordinal-data-materials-procedure}



#### Outcome (dependent) variables {#sec-ordinal-data-outcome}

### Locate and download the data file {#sec-ordinal-data-download}

You can download the [2021-22_PSYC304-health-comprehension.csv](files/2021-22_PSYC304-health-comprehension.csv) file holding the data we analyse in this chapter by clicking on the link.

### Read-in the data file using read_csv {#sec-ordinal-data-import}

I am going to assume you have downloaded the data file, and that you know where it is. We use `read_csv` to read the data file into R.

<!-- ```{r} -->

<!-- #| label: readinall -->

<!-- #| message: false -->

<!-- long.orth <- read_csv("long.orth_2020-08-11.csv",  -->

<!--                       col_types = cols( -->

<!--                         Participant = col_factor(), -->

<!--                         Time = col_factor(), -->

<!--                         Study = col_factor(), -->

<!--                         Instructions = col_factor(), -->

<!--                         Version = col_factor(), -->

<!--                         Word = col_factor(), -->

<!--                         Orthography = col_factor(), -->

<!--                         Measure = col_factor(), -->

<!--                         Spelling.transcription = col_factor() -->

<!--                       ) -->

<!--                     ) -->

<!-- ``` -->

### Inspect the data {#sec-ordinal-data-inspect}

It is always a good to inspect what you have got when you read a data file in to R.

<!-- ```{r} -->

<!-- #| label: data-summary -->

<!-- #| eval: false -->

<!-- summary(long.orth) -->

<!-- ``` -->

## Tidy the data {#sec-ordinal-data-tidy}

### Code categorical factors {#sec-ordinal-data-recode}

<!-- ```{r} -->

<!-- #| label: glm-summary -->

<!-- summary(glm(Score ~ Orthography, family = "binomial", data = long.orth)) -->

<!-- ``` -->

::: callout-tip
-   
:::

## Introduction to thinking about the need for generalized models {#sec-ordinal-ideas-intro}

## Our focus is on the analysis of catgorical outcome variables {#sec-ordinal-categorical-outcomes}

### Recognize the limitations of alternative methods for analyzing response accuracy {#sec-tradition-limitations}

::: callout-tip
-   
:::

#### Accuracy is bounded between 1 and 0, linear model model predictions or confidence intervals are not {#sec-ordinal-bounded-outcomes}

#### ANOVA or regression require the assumption of homogeneity of variance but for binary outcomes like accuracy the variance is proportional to the mean {#sec-ordinal-anova-assumptions}

#### Summary: Recognize the limitations of traditional methods for analyzing response accuracy {#sec-ordinal-limitations-summary}

### Understanding the Generalized part of the Generalized Linear Mixed-effects Models in practical terms {#sec-ordinal-practical-understanding}

## Working with Cumulative Link Mixed-effects Models in R {#sec-ordinal-working-models}

### Specify a random intercepts model {#sec-ordinal-random-intercepts}

In our first model, we will specify just random effects of participants and items on intercepts.

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- long.orth.min.glmer <- glmer(Score ~  -->

<!--                                Time + Orthography + Instructions + zConsistency_H +  -->

<!--                                Orthography:Instructions + -->

<!--                                Orthography:zConsistency_H + -->

<!--                                (1 | Participant) +  -->

<!--                                (1 | Word), -->

<!--                              family = "binomial",  -->

<!--                              glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), -->

<!--                              data = long.orth) -->

<!-- summary(long.orth.min.glmer) -->

<!-- ``` -->

The code works as follows.

First, we have a chunk of code mostly similar to what we have done before, but changing the function.

-   `glmer()` the function name changes because now we want a *generalized* linear mixed-effects model of accuracy.

The model specification includes information about fixed effects and about random effects.

-   With `(1 | Participant)` we include random effects of participants on on intercepts.
-   With `(1 | Word)` we include random effects of stimulus on on intercepts.

Second, we have the bit that is specific to *generalized* models.

-   `family = "binomial"` is entered because accuracy is a binary outcome variable (correct, incorrect) so we assume a binomial probability distribution.

We then specify:

-   `glmerControl(optimizer="bobyqa", ...)` to change the underlying mathematical engine (the optimizer) to cope with greater model complexity\
-   and we allow the model fitting functions to take longer to find estimates with `optCtrl=list(maxfun=2e5)`.

Notice how we specify the fixed effects. We want `glmer()` to estimate "main effects and interactions" that we hypothesized.

We specify the *main* effects with:

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- Time + Orthography + Instructions + zConsistency_H + -->

<!-- ``` -->

We specify the *interaction* effects with:

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- Orthography:Instructions + -->

<!-- Orthography:zConsistency_H + -->

<!-- ``` -->

### Read the results {#sec-ordinal-results}

If you run the model code, you will get the results shown in the output.

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- long.orth.min.glmer <- glmer(Score ~  -->

<!--                                Time + Orthography + Instructions + zConsistency_H +  -->

<!--                                Orthography:Instructions + -->

<!--                                Orthography:zConsistency_H + -->

<!--                                (1 | Participant) +  -->

<!--                                (1 |Word), -->

<!--                              family = "binomial",  -->

<!--                              glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), -->

<!--                              data = long.orth) -->

<!-- summary(long.orth.min.glmer) -->

<!-- ``` -->

#### Ordinal models and hypothesis tests {#sec-ordinal-hypothesis}

### Presenting and visualizing the effects {#sec-ordinal-visualizing}

## Examining if we should include random effects {#sec-ordinal-random-effects}

#### Getting started {#sec-ordinal-random-effects-start}

::: callout-tip
The question is:

-   So, what random effects should we include?
:::

#### Random effects of subjects and stimulus items on intercepts {#sec-ordinal-random-effects-subjects-items-intercepts}

#### Random effects of subjects and stimulus words on the slope of the Orthography effect {#sec-ordinal-random-effects-subjects-items-slopes}

#### Random effects of subjects and stimulus words on the slope of the Instructions effect {#sec-ordinal-random-effects-subjects-items-slopes-2}

#### Adding effects a bit at a time {#sec-ordinal-random-effects-add}

#### Random effects of subjects on the slope of the consistency effect {#sec-ordinal-random-effects-subjects-items-slopes-3}

#### Random effects of subjects and stimulus words on the slope of the time effect {#sec-ordinal-random-effects-subjects-items-slopes-4}

### Bad signs {#sec-ordinal-bad-signs}

### Comparison of models varying in random effects {#sec-ordinal-comparing0-models-2}

### Addressing convergence problems {#sec-ordinal-addressing-convergence}

#### Exercises

#### Summary advice {#sec-ordinal-advice-summary}

## Reporting model results {#sec-ordinal-reporting-results}

## Summary {#sec-ordinal-summary}

### Glossary: useful functions {#sec-ordinal-glossary-useful-functions}

## Recommended reading {#sec-ordinal-recommended-reading}

The published example studies referred to in this chapter are published in [@ricketts2021; @rodríguez-ferreiro2020a].

@liddell2018 present a clear account of the problems associated with treating ordinal data as metric, and explain how we can better account for ordinal data.

@bürkner2019 present a clear tutorial on cumulative and sequential ratio models.

Both @liddell2018 and @bürkner2019 work from a Bayesian perspective but the insights are generally applicable.

Guides to the `{ordinal}` model functions `clm()` and `clmm()` are presented in [@christensen2022; @christensen2015].
